\pagestyle{empty}
\chapter*{Resumen}
\addcontentsline{toc}{chapter}{\protect{Resumen}}

\markboth{Resumen}{Resumen}

\pagestyle{headings}
 
Los sistemas de información están experimentando una gran expansión debido, en gran medida, al enorme crecimiento de la cantidad de información que se genera en la sociedad actual. Es por tanto un reto el investigar técnicas que permitan sacar partido y gestionar tales cantidades de información. En este marco, existen numerosos campos de conocimiento como pueden ser: sistemas inteligentes, big data \cite{Miao2014}, cloud computing, etc. Concretamente, esta tesis doctoral se sitúa en el ámbito del tratamiento de la información desde el punto de vista del análisis de conceptos formales (FCA), con la intención de aplicarlo en ámbitos diferentes de la ingeniería informática como son las bases de datos y los sistemas de recomendación.

FCA es una teoría matemática y un método para el análisis de datos en cuanto a sus relaciones y estructura. El cometido principal de FCA es definir un método basado en las matemáticas y la lógica que corresponda al pensamiento conceptual del ser humano. Es una técnica con una sólida base formal \cite{Ganter1997} que puede ser aplicable a un conjunto variado de disciplinas como pueden ser: data mining \cite{Stumme2002}, sistemas inteligentes, machine learning \cite{Mitchell1997}, web semántica \cite{Wang2006}, desarrollo software, etc. FCA se ha establecido como una alternativa sólida al proceso de la adquisición y tratamiento de la información para su posterior uso en aplicaciones usando técnicas de razonamiento automático. Más concretamente, se ha visto reconocido mediante un aumento muy significativo de aplicaciones en áreas muy diversas entre las que podemos destacar la biomedicina, turismo, educación, web semántica, desarrollo de software, biología, redes sociales, etc. Gran parte de este creciente interés radica en disponer de un marco único en el que poder desarrollar de principio a fin el ciclo que nos lleva de la información al conocimiento y, sobre él, poder realizar tareas de razonamiento automático.

Hay diversas técnicas usadas con mucho éxito en tareas de minería de datos, clasificación, etc. Pero una vez llevadas a cabo estas tareas, debemos hacer uso de otras áreas para trabajar con el conocimiento adquirido. Sin embargo, una gran diferencia entre gran parte de tales aproximaciones y FCA es que ésta última busca recopilar todo el conocimiento existente en el sistema. Así, mientras en otras áreas el término aproximado es habitual, aquí nos encontramos con un requerimiento importante en cuanto a la completitud semántica. Evidentemente, este objetivo conlleva una gran complejidad y es natural que el sistema se vea representado por retículos de conceptos de gran tamaño o por sistemas de implicaciones igualmente grandes. Sin embargo, se ha hecho un gran esfuerzo últimamente en el desarrollo de técnicas y métodos eficientes que han allanado el terreno en el marco del FCA clásico.

%Nuevas aplicaciones, como pueden ser los sistemas de comercio electrónico \cite{Guo2014}, están demandando una mayor potencia expresiva que les permita una interacción más cercana para con el usuario final. Así por ejemplo, queremos poder agilizar un supuesto sistema de diagnóstico, de modo que a partir de una reducida información proporcionada por un paciente, seamos capaces de proponer un primer diagnóstico general; o bien, si buscamos hospedarnos en un hotel cercano a la celebración de algún evento, haremos hincapié en encontrar una buena ubicación, mientras que la calidad que presenten servicios de ocio del hotel tendrá una menor importancia. Estos propósitos implican un desarrollo nuevo desde la extracción de implicaciones hasta las lógicas que las manejan.

A partir de la información disponible, las relaciones lógicas que se obtienen son aquellas que vengan confirmadas por nuestros contextos formales, que podemos considerar como tablas de datos que relacionan la información existente entre un conjuntos de objetos y un conjunto de atributos. Principalmente, existen dos formas básicas para representar el conocimiento extraído de estas tablas: los retículos de conceptos y los conjuntos de implicaciones. 

Desde hace años, existen en la literatura estudios \cite{Kuznetsov2002} donde se han investigado y comparado diferentes algoritmos para obtener el retículo de conceptos a partir del conjunto de datos. La dificultad de utilizar retículos de conceptos se debe a que su tamaño puede alcanzar cotas que, en el peor de los casos, supongan un coste computacional muy elevado para los métodos encargados de su construcción, lo cual constituye una limitación en la aplicación de FCA.

Por otro lado, las implicaciones pueden considerarse como reglas del tipo \textit{si-entonces}, que reflejan un concepto muy intuitivo: cuando se verifica una premisa, entonces se cumple una conclusión. Consecuentemente, en FCA podemos formalizarlo de la siguiente forma: dados $A$ y $B$ subconjuntos de atributos, diremos que se tiene la implicación $A \rightarrow B$ si se verifica $A' \in B'$, es decir, todos los objetos que tienen cada atributo de $A$ también tienen cada atributo de $B$. Con esta definición, las implicaciones obedecen los Axiomas de Armstrong \cite{Armstrong74} (reflexivo, aumentativo y transitivo) comunes en las dependencias funcionales que se dan entre los atributos de una base de datos y considerados como el primer sistema de inferencia para dependencias funcionales completo. Trabajar con conjuntos de implicaciones nos permite aplicar técnicas automáticas basadas en la lógica. Este hecho se alza como el objetivo principal de esta tesis doctoral, que principalmente consiste en aplicar mecanismos lógicos sobre conjuntos de implicaciones que nos permitan desarrollar métodos para realizar un tratamiento eficiente de la información.

La aproximación a través de la lógica es posible, en nuestro caso, gracias a sistemas axiomáticos válidos y completos como los mencionados Axiomas de Armstrong \cite{Armstrong74} y la Lógica de Simplificación \cite{Mora2004}. Serán estos métodos los que, aplicados sobre conjuntos de implicaciones, nos proporcionen la base para trabajar sobre los siguientes tres campos de conocimiento: claves minimales, generadores minimales y sistemas de recomendación conversacionales.


\subsection*{Objetivos}
Se ha pretendido que la tesis implique unas contribuciones que reflejen la importancia de la información contenida dentro de los datos, la necesidad de su tratamiento eficiente y cómo los mecanismos teóricos de FCA y la lógica de simplificación son herramientas ideales para abordar estos problemas. Concretamente, nos hemos centrado en el estudio de sistemas de implicaciones basados en FCA que permiten mejorar el tratamiento de la información contenida en repositorios de datos, bases de datos, etc., y en cómo las técnicas así desarrolladas permiten mejorar aspectos relacionados con las bases de datos y los sistemas de recomendación. En concreto, respecto a las bases de datos, este conjunto de técnicas nos van a facilitar el descubrimiento de las claves y generadores minimales de un conjunto de datos. Por otro lado, respecto a los sistemas de recomendación, mediante el uso de estas técnicas de tratamiento inteligente del conjunto de implicaciones de un determinado bloque de información, vamos a ser capaces de aligerar el proceso de recomendación y mejorar la interacción con el usuario aliviando la sobrecarga de información.

Por tanto, a lo largo de la tesis, se han investigado e implementado los métodos teóricos con el objetivo de verificar su eficiencia y su adaptabilidad a modelos reales en bases de datos y sistemas de recomendación. En particular, podemos decir que el objetivo fundamental de esta tesis consiste en aproximar problemas candentes en el ámbito de la manipulación de información mediante métodos que aprovechen el concepto de sistema de implicaciones. Estos sistemas presentan las siguientes características generales:
\begin{itemize}
	\item Son una herramienta óptima en cuanto al ratio expresividad/coste computacional.
	\item Permiten utilizar mecanismos de lógica.
	\item Pueden ser extendidos (difusos \cite{Belohlavek2016}, etc.).
	\item Permiten ser tratados en el ámbito de FCA.
\end{itemize}

En este sentido, tenemos que indicar que el desarrollo de la tesis ha conllevado la consecución de objetivos de forma teórica y práctica de forma simultánea. Podemos desglosarlos de la siguiente manera:

\begin{itemize}
	\item Teóricos. 
	\begin{itemize}
		\item Obtención de nuevos métodos de tratamiento de la información mediante implicaciones y FCA, así como realizar avances sobre los ya existentes, que permiten realizar una gestión más inteligente y eficaz de la información.
		\item Investigación y propuesta de nuevas estrategias para la extracción de claves y generadores minimales a partir de tablas de datos.
		\item Investigación y propuesta de nuevas estrategias de recomendación que mejoren la calidad de las recomendaciones y la interacción con el usuario.
	\end{itemize}
	\item Prácticos.
	\begin{itemize}
		\item Desarrollar las implementaciones y pruebas necesarias con los métodos teóricos de tratamiento de información basados en implicaciones que permitan verificar su funcionamiento sobre grandes cantidades de información.
		\item Desarrollar SR que se beneficien de estos métodos y sean aplicables al entorno tecnológico actual.
	\end{itemize}
\end{itemize}
 
Dada la naturaleza de los métodos desarrollados, para la realización de grandes pruebas de carga como son las necesarias para probar las implementaciones desarrolladas, será necesario contar con unos recursos inusualmente elevados en términos computacionales. Por esa razón, se ha efectuado una profunda labor de interacción con el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga, de forma que los recursos allí presentes nos han permitido realizar las pruebas necesarias.

Previamente a entrar a describir de forma general el balance de aportaciones conseguidas, en este punto, creemos adecuado establecer un breve compendio de las disciplinas que han tenido que confluir para dar lugar a la elaboración de esta tesis doctoral. En primer lugar, aparece la Lógica como base fundamental donde sustentar el estudio. Además, existe una considerable intervención de las Matemáticas, no sólo como pilar para la base lógica, sino también como mecanismo para llevar a cabo gran cantidad de cálculos y aproximaciones a la hora de preparar los experimentos realizados. A las dos anteriores, y de forma evidente dada la naturaleza de la tesis, se suma la Ingeniería Informática, interviniendo de forma global en los diversos aspectos que han conducido la investigación a la práctica. Por un lado, existe una importante labor de ingeniería del software que ha permitido desarrollar los códigos necesarios para poner en el práctica los métodos teóricos y para conseguir los resultados que han conducido a las aportaciones. En especial, debemos mencionar la labor de programación desarrollada en entornos de computación de alto rendimiento utilizando lenguajes de bajo nivel. Adicionalmente, se ha producido una profunda inmersión en el campo de los Sistemas de Información, en concreto, aquellos relacionados tanto a bases de datos como a sistemas de recomendación. Por tanto, la interacción conjunta de aspectos provenientes de todas estas disciplinas ha sido lo que ha conducido a culminar esta investigación con los resultados que se presentan en esta tesis doctoral reflejados en el apartado de aportaciones\ref{cap:aportaciones}.


\subsection*{Contribuciones al campo de las claves y los generadores minimales}
\noindent
En la primera parte de la tesis, vamos a estudiar dos campos de conocimiento que, si bien diferentes, están fuertemente relacionados: claves minimales y generadores minimales. Comenzamos por el tema de las claves minimales.

\vspace{0.3cm}

Encontrar todas las claves minimales en una tabla es un problema difícil que proporciona muchos beneficios en el diseño y la optimización de las bases de datos. Identificar adecuadamente las claves de un esquema relacional es una tarea crucial para muchas áreas actuales de gestión de la información como pueden ser el modelado de datos, optimización de consultas, indexado, etc. El concepto de clave es fundamental en cualquier modelo de datos, incluyendo el modelo de datos relacional de Codd \cite{Codd1970}.

Las claves nos especifican conjuntos de atributos de una relación tal que su proyección identifica unívocamente cada tupla de la relación. Una característica muy importante de las claves es su minimalidad. Denotamos una clave como minimal cuando cada atributo contenido en su conjunto de atributos es necesario para mantener la propiedad de clave, es decir, son claves sin atributos superfluos. Por lo tanto, en la literatura, el problema de las claves minimales consiste en encontrar todos los subconjuntos de atributos que conforman una clave minimal dado un conjunto de dependencias funcionales que se producen dentro de una tabla relacional.

%Por tanto, cada clave está compuesta por un subconjunto de atributos que desempeña el papel de un {\em dominio} de una función dada cuya {\em imagen} es el conjunto completo de atributos. En este campo, estas funciones, que en FCA aparecen como implicaciones, se describen por medio de dependencias funcionales que especifican una restricción entre dos subconjuntos de atributos $A$ y $B$, se denota como $ A\to B $, y reflejan una idea muy intuitiva, para cualesquiera dos tuplas en una tabla, si verifican $A$, también verifican $B$.

Desde hace años existen varios algoritmos que utilizan diferentes técnicas clásicas para abordar este problema \cite{Lucchesi78,Yu76,Zhang09,Armstrong74}. Más recientemente, han aparecido métodos alternativos utilizando la lógica. En esta tesis nos concentraremos en esos algoritmos guiados por la lógica, en especial por la lógica \slfde \cite{Enciso2002}, y más específicamente, aquellos que usan el paradigma de Tableaux \cite{Morgan1992,Risch1992} para encontrar las claves de un esquema relacional utilizando sistemas de inferencia. %El paradigma de Tableaux puede considerarse un marco flexible y potente para diseñar métodos de deducción automatizados para resolver problemas complejos de una manera eficiente.

En trabajos anteriores, se han introducido varios métodos basados en Tableaux \cite{Wastl98a,Wastl98}. Sin embargo, estos métodos generan amplios espacios de búsqueda y, en muchos casos, la misma solución (la misma clave minimal) aparece al final de varias ramas del árbol que representa el espacio de búsqueda. Estas características intrínsecas de los métodos basados en Tableaux generan una fuerte limitación respecto al tamaño de los problemas que pueden tratarse, ya que para problemas grandes, los espacios de búsqueda pueden ser inmanejables como se ha demostrado en trabajos anteriores \cite{Cordero2013}.

Sin embargo, una propiedad muy interesante dentro de los espacios de búsqueda inducidos por los métodos de Tableaux, es la total independencia de sus ramas, por lo que cada nodo del árbol de búsqueda podemos considerarlo como un subproblema independiente del problema original. Gracias a ello, trabajar con estos árboles de búsqueda nos proporciona un camino óptimo para construir algoritmos paralelos capaces de encontrar todas las claves minimales de una tabla mediante un procesamiento paralelo e independiente.

De esta forma, nuestro objetivo ha sido diseñar e implementar algoritmos de búsqueda de claves minimales basados en Tableaux que admitieran cantidades considerables de información a la entrada. Para ello, se han implementado códigos capaces de funcionar sobre arquitecturas de recursos hardware de forma paralela. Para realizar esta tarea de computación paralela hemos necesitado utilizar recursos computacionales especiales. En concreto, hemos hecho uso de los recursos disponibles en el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga\footnote{https://www.scbi.uma.es/} sin los cuales hubiera sido imposible realizar la gran mayoría de experimentos ya que, en muchos casos, las soluciones se han obtenido tras grandes periodos de tiempo de computación masiva como podemos apreciar en \cite{Benito-Picazo2014,Benito-Picazo2016}.

En este sentido, nuestra contribución principal se produce de la siguiente forma. Basándonos en el sistema axiómatico de la lógica \slfde \ref{sec:logicaSimplificacion}, proponemos un nuevo método para la búsqueda de claves minimales, denominado \textit{Closure Keys} (CK) \ref{subsec:metodoCK}, que incorpora un mecanismo eficiente de poda que utiliza el método de cierre basado en \slfde para mejorar el método SST \cite{CorderoEMG14}. El nuevo método se basa en la relación fuerte entre la noción de clave y el operador de cierre definido en \cite{Mora2012}, el cual nos permite reducir el espacio de búsqueda realizando reducciones en el camino hacia las hojas, donde finalmente se obtienen las claves. Además, se llevan a cabo  varios experimentos entre las implementaciones paralelas del método SST y del método CK (en entornos de supercomputación), para confirmar las mejoras que se han alcanzado, que principalmente son una significativa reducción del número de nodos del árbol de búsqueda y de los tiempos de ejecución como podemos apreciar en las contribuciones realizadas en \cite{Benito-PicazoCMMSE2015,Benito-Picazo2016}.

\vspace{0.3cm}

Pasamos ahora a recopilar las contribuciones realizadas en referencia al campo de los generadores minimales.

Los conjuntos cerrados y los generadores minimales son elementos fundamentales para construir una representación completa del conocimiento en FCA. Nuestra intención ha sido mostrar cómo utilizar el sistema de inferencia de \slfde \cite{Enciso2002,Cordero2012} y la salida del algoritmo  del cierre \cierree como alternativa para enumerar todos los conjuntos cerrados y todos los generadores minimales de un conjunto de atributos y de implicaciones de entrada. 

Para ello, se han realizado las implementaciones del método de cálculo de generadores minimales y de dos evoluciones de éste, en las que una nueva estrategia de poda va a propiciar que los resultados obtenidos en relación al tiempo de ejecución y la magnitud del árbol de búsqueda mejoren sobremanera. Además, de forma análoga al caso de las claves minimales en cuanto a poder tratar con cantidades sustanciales de información, se han realizado versiones de los métodos de forma que puedan resolverse mediante técnicas de computación masiva basadas en estrategias paralelas en el entorno de computación de alto rendimiento que nos ofrece el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga\footnote{https://www.scbi.uma.es/}.

El cálculo de los generadores minimales es un problema que no se reduce al ámbito teórico sino que se utiliza para resolver multitud de problemas. Como consecuencia, existen en la literatura amplios estudios sobre generadores minimales de los que podemos destacar \cite{Poelmans2013,Qu2007,Missaoui2012}.

Para $X,Y \subseteq M$ que satisface $X = Y^+_\Sigma$, se dice que $Y$ es un generador del conjunto cerrado $X$. Como consecuencia, cualquier subconjunto de $X$ que contenga $Y$ es también un generador de $X$. Como vamos a trabajar con conjuntos de atributos finitos, el conjunto de los generadores de un conjunto cerrado se pueden caracterizar por aquellos que sean minimales. Sin embargo, la enumeración de todos los conjuntos cerrados y sus generadores minimales a partir de un conjunto implicaciones constituye un problema complejo con un coste exponencial.

Por nuestra parte, vamos a hacer uso de los conjuntos de implicaciones como base de un método para enumerar todos los conjuntos cerrados y sus generadores minimales. Este método utiliza la lógica \slfde y constituye una mejora considerable del mostrado en \cite{Cordero2012}. El método genera un árbol de búsqueda, utilizando la misma filosofía de Tableaux de las claves minimales, donde se van reflejando los conjuntos cerrados y sus generadores minimales. Este método recibe el nombre de \ref{algoritmo:minGen} y es la semilla para elaborar dos nuevas versiones mejoradas. En primer lugar, el método \ref{algoritmo:minGenPr} incorpora un mecanismo de poda para evitar la generación de generadores minimales y cierres redundantes, y en segundo lugar, \ref{algoritmo:GenMinGen} propone una generalización de la estrategia de poda anterior al considerar el test de inclusión del subconjunto no sólo con la información de los nodos del mismo nivel, sino también con todos los generadores minimales calculados hasta ese momento.

Para demostrar el rendimiento de los métodos, se han implementado los códigos necesarios para cada uno de ellos y se ha procedido a resolver una batería de problemas de entrada para poder el rendimiento logrado por cada uno de ellos. Para evaluar esta comparación, vamos a utilizar las mismas dos métricas que para el caso de las claves minimales, es decir, el tiempo de ejecución del algoritmo y el número de nodos en el árbol de búsqueda generado; y la justificación es idéntica que para claves minimales.

Después de realizar estos experimentos que se pueden resolver de forma local en una máquina convencional debido a la baja cantidad de información que contiene los ficheros originales, los resultados nos dicen que \ref{algoritmo:minGen} es superado por \ref{algoritmo:minGenPr}, y a su vez \ref{algoritmo:GenMinGen} mejora este último.

Los números obtenidos para las resoluciones secuenciales de los algoritmos son muy prometedores. Sin embargo, al igual que en el caso de claves minimales, queremos ser capaces de aumentar el tamaño de los problemas de entrada y para ello, volvemos a introducirnos en tareas de computación paralela. De esta forma, se realizan versiones paralelas de los códigos de los métodos y se prueban en entornos de supercomputación, demostrando cómo el paralelismo es claramente apropiado para resolver este tipo de problemas y cómo nos permite trabajar con ficheros con una mayor cantidad de información. Para sustentar estas declaraciones sobre los resultados obtenidos, se incluye una serie de tablas y figuras donde poder apreciar fácilmente las mejoras alcanzadas. Las contribuciones realizadas para esta parte las podemos encontrar en \cite{Benito-PicazoCMMSE2017}.






\subsection*{Contribuciones al campo de los sistemas de recomendación conversacionales}
\noindent
En la segunda parte de la tesis, vamos a introducirnos en el campo de los SR, y en especial, en el tipo de SR denominado conversacional \cite{Griol2018,Lee2017}. Haremos uso de los conceptos de FCA y en especial de los conjuntos de implicaciones y los operadores de cierre para lograr un desarrollo de SR y abordar uno de los problemas más comunes que aparecen en estos sistemas. 

De forma muy básica, podríamos considerar a los SR como herramientas que agrupan un amplio abanico de técnicas y aplicaciones de los sistemas de información con la intención de potenciar y favorecer la experiencia de usuario mediante recomendaciones. Es un campo de conocimiento con un amplio recorrido desde décadas \cite{Hill1995,Adomavicius2005} pero que actualmente está consiguiendo una relevancia muy considerable debido sobre todo a la expansión de las nuevas tecnologías que está propiciando un mayor acercamiento a la mayoría de la sociedad debido a su capacidad para realizar todo tipo de recomendaciones sobre diversos elementos al alcance de todos (libros, música, empleos, etc.).

Existe una gran cantidad de tipos de SR, normalmente agrupados según la estrategia de recomendación que utilizan. Entre estos tipos podemos nombrar los SR: basados en contenido, basados en conocimiento, contextuales, grupales, conversacionales, de filtrado colaborativo, entre varios otros. Podemos apreciar una clasificación más detallada en el libro de Adomavicius y Tuzhilin \cite{AdomaviciusBook11}, el cual podemos considerar como una de las referencias más importantes del campo.

Del mismo modo que existe tal abanico de posibilidades en cuanto a los tipos de SR que existen, es natural que la cantidad de problemas a los que tienen que enfrentarse estos sistemas, tanto en su fase de desarrollo como de aplicación, sea numerosa. Podemos recopilar una lista con los más comunes tales como: arranque en frío, privacidad, oveja-negra, escasez, diversidad, etc. En esta lista podemos incluir un problema muy común del que adolecen gran parte de los SR, es el denominado problema de la alta dimensionalidad \cite{Salimi2017,Nagler2016}. Este problema aparece en los SR cuando es necesario trabajar sobre \textit{datasets} con un alto número de elementos. Básicamente, podemos decir que este problema refleja la dificultad existente para elaborar una recomendación cuando son muchos los elementos entre los que hay que decidir, pero también cuando son muchas las características que cada elemento puede cumplir. 

Como ejemplo intuitivo, supongamos que queremos elegir un restaurante para cenar pero tenemos una lista de 100 locales disponibles, de los cuales cada uno de ellos además puede ofrecer una lista enorme de servicios diferentes (tipo de cocina, terraza al aire libre, comedor, música en directo, acceso minusválidos, precio, etc.).

Para abordar este problema, podemos encontrar muchos trabajos en la literatura sobre la reducción de la dimensión de la información que pueden ayudarnos a descartar aquellas características que no son merecedoras de ser considerados según diferentes criterios. En \cite{Chen2007,Jannach2009,TrabelsiWBR11}, se establecen enfoques basados en el conocimiento para los procesos conversacionales, se tiene en cuenta el número de pasos de la conversación, se demuestra que si el usuario es el encargado de la selección de atributos, los resultados son mejores que si es el sistema mismo el encargado de dicha selección. Este hecho respalda nuestro enfoque en el cual el humano experto guía la conversación y el proceso de selección de características.

Por nuestra parte, el objetivo ha sido abordar el problema de la alta dimensionalidad en los SR a través de un proceso de selección de atributos por parte del usuario mediante un SR conversacional que utilice características de los SR basados en contenido y en conocimiento. Para ello el SR desarrollado (denominado \rs) hará un tratamiento eficiente de la información haciendo uso de FCA, las implicaciones y los operadores de cierre. En concreto haremos uso de la \slfde \cite{Enciso2002} y del algoritmo del cierre \ref{algoritmo:Cls}.

\vspace{0.3cm}

Para poder utilizar \rs, hay que tener en cuenta que lo primero que vamos a necesitar son \textit{datasets} con elementos sobre el que poder hacer recomendaciones. Pero además, estos \textit{datasets} tendrán que contener la información según una representación binaria, es decir, el valor de cada uno de los atributos asociados a cada elemento del sistema será un valor binario que representará si ese elemento verifica ese atributo o no lo hace. Una vez tengamos un \textit{dataset} en ese formato, necesitamos el conjunto de implicaciones que se verifican en los datos y para ello se hará uso de herramientas ya existentes como son \cite{HuhtalaKPT99,YaoHB2002,Yevtushenko2006}. Una vez tengamos tanto \textit{dataset} como conjunto de implicaciones, ya proceder a utilizar \rs.

Dada la naturaleza conversacional de \rs, el proceso de recomendación para el usuario se desarrolla por medio de un diálogo. En éste, el usuario irá solicitando características que quiere que cumpla el elemento que busca (e.g. precio moderado, si busca una bicicleta; cocina sueca, para un restaurante; resolución en megapíxels, si busca un cámara de fotos, etc.). Tras la selección de característica por parte del usuario, el sistema ejecuta su algoritmo que realiza dos tareas fundamentales, calcular el conjunto cierre correspondiente a la selección del usuario y calcular el conjunto de implicaciones restante. Al terminar y tras lanzar una consulta a base de datos, el SR propone una recomendación. El usuario puede aceptar la recomendación o continuar el proceso añadiendo nuevas características que refinen el resultado. Sin embargo, a cada paso, no tendrá que decidir sobre todo el conjunto de posibilidades original, sino que ahora, sólo tendrá que navegar entre aquellas que vayan quedando tras la aplicación del algoritmo, es decir, las que no estén lógicamente implícitas por la selección actual; aliviando la sobrecarga de información al usuario.

El gran logro de usar \rse con el algoritmo \cierree es que estamos reduciendo el número de características o atributos y el número de implicaciones \textit{simultáneamente}, y por tanto, en cada nuevo paso del diálogo no necesitamos volver a extraer el nuevo conjunto de implicaciones, lo cual es una tarea de \textit{data-mining} con coste exponencial, sino que podemos continuar la interacción a partir de aquí, donde tanto atributos como implicaciones han sido reducidas, y por tanto con coste \textit{lineal}.

Desde que se inició la investigación de los SR, la evaluación de las predicciones y recomendaciones se ha convertido en un aspecto muy importante \cite{Herlocker2004,Burke2010}. La investigación en el campo de los SR requiere medidas de calidad y métricas de evaluación \cite{Gunawardana2009} para conocer la calidad de las técnicas, métodos y algoritmos para las predicciones y recomendaciones. Las métricas de evaluación \cite{HernandezdelOlmo2008} y los frameworks de evaluación \cite{Bobadilla2011} facilitan la comparación de varias soluciones para el mismo problema. Teniendo en cuenta la gran variedad de factores que pueden intervenir en el funcionamiento de los SR, la primera tarea a llevar a cabo es evaluar las opciones para averiguar qué métricas son las que mejor se adaptan al SR que queremos evaluar.

En nuestro caso, dado que estamos tratando con un SR conversacional, puede ser evidente que la primera medida que podemos aplicar es calcular el número de pasos que se producen en la conversación \cite{McSherry01}. Por contra, otras métricas tan populares en los SR como son \textit{Precision}, \textit{Recall} \cite{Gunawardana2015}, MAE, RMSE, etc., no son adecuadas de aplicar en nuestro caso porque obtendríamos siempre valores máximos en ambas métricas. Esto se debe a que estamos tratando con implicaciones, y por tanto, no estamos ante un método aproximando sino que se está asegurando que la fiabilidad es del 100\%.

Por ello, además de calcular el número de pasos de la conversación, se han definido dos medidas de evaluación adicionales para medir el rendimiento del sistema, con lo que la lista de medidas de evaluación utilizadas sobre \rse queda como sigue:
\begin{enumerate}
	\item \textbf{Número de pasos.} Registra el número de pasos que se dan en el diálogo y en consecuencia, el número de atributos seleccionados por el usuario. Nos proporciona una visión clara de si el sistema ha necesitado mucha o poca interacción para satisfacer la demanda del usuario.
	\item \textbf{Velocidad de poda en cada paso del diálogo.} Evalúa el porcentaje de atributos que el sistema libera al usuario de tener en cuenta a lo largo de la conversación y de forma acumulativa de un paso al siguiente. Refleja si los ratios de poda del algoritmo son mejores al principio de la conversación o en los pasos posteriores
	\item \textbf{Reducción de atributos.} Informa de la reducción global de atributos que ha realizado el sistema al terminar el diálogo.
\end{enumerate}

Una vez tuvimos el sistema definido junto con su funcionamiento y las métricas de evaluación que se van a usar, pasamos a la elaboración de una amplia serie de experimentos con los que comprobar la viabilidad, la utilidad y el beneficio que obtenemos con el trabajo desarrollado \ref{cap:experimentosSR}. 

El capítulo de experimentos \ref{cap:experimentosSR} es realmente importante por varias razones. La primera es, obviamente, dar validez al estudio y el desarrollo realizado; pero los experimentos llevados a cabo también nos sirven para poner de manifiesto el recorrido efectivo desde la parte teórica hasta la práctica que ha finalizado con una aplicación real de ingeniería.

Se han efectuado experimentos que incluyen tanto información, podríamos llamar de prueba, como información real. Los resultados demuestran cómo el sistema desarrollado cumple con su cometido y además, sobrepasa otras propuestas existentes en la literatura \cite{TrabelsiWBR11}. En este sentido, se ha trabajado y evaluado \rse sobre \textit{datasets} que han sido un referente a la hora de probar el funcionamiento de estos sistemas como son los \textit{datasets} de MovieLens.

Adicionalmente, ya que muchos de los \textit{datasets} disponibles en la web necesitaban un cierto tratamiento previo para poder adaptarse al formato aceptado por \rs, se optó por construir \textit{datasets} propios a partir de información disponible en la web. Para ello se crearon \textit{datasets} como son Hoteles Costa del Sol \ref{seccion:hotelesCostaDelSol}, POIs Mundial \ref{seccion:POIsMundial}, o Enfermedades y Síntomas \ref{seccion:sintomasDataset}, este último utilizado en una de las contribuciones que avalan la tesis \cite{Benito-Picazo2017}.

Sobre cada uno de ellos se realizó un test consistente en simular 100 diálogos entre el usuario y el sistema donde se aplicaron las métricas mencionadas y se recogieron los resultados obtenidos. En cada caso, se hace un breve compendio a modo de conclusión donde se recoge las mejoras alcanzadas y se justifica el progreso sobre otras alternativas. 

Principalmente, podemos afirmar que hemos conseguido dar un paso adelante para abordar el problema de la alta dimensionalidad que aparece en los SR gracias al tratamiento que realizamos sobre los datos mediante el uso de los conjuntos de implicaciones y los operadores de cierre.










