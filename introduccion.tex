\pagestyle{empty}
\chapter{Introducción}\label{cap:introduccion}
%\addcontentsline{toc}{chapter}{\protect{Introduction}}

%\markboth{Introduction}{Introduction}

\pagestyle{headings}

\bigdrop{0pt}{5}{cmr10}La gestión de la información es uno de los pilares esenciales de la Ingeniería Informática. No es de extrañar, por tanto, que conforme un amplio campo de investigación y conocimiento donde diversas disciplinas como las Matemáticas, la Lógica y la Ingeniería actúen conjuntamente para alcanzar mejores sinergias.

Dentro de esta filosofía y en aras de mejorar el funcionamiento de dos tipos de sistemas de información como son las bases de datos y los sistemas de recomendación, en esta tesis doctoral nos vamos a centrar en el Análisis Formal de Conceptos (FCA, por sus siglas en inglés: \textit{Formal Concept Analysis}); concretamente, en la utilización de dos de sus conceptos fundamentales: los retículos de conceptos y los sistemas de implicaciones. La gestión inteligente de estos elementos mediante técnicas lógicas y computacionales confieren una alternativa para superar obstáculos que podemos encontrar a la hora de trabajar con los sistemas de información mencionados. Comenzamos pues introduciendo brevemente FCA.

Podemos considerar FCA como una teoría matemática y una metodología para derivar una jerarquía de conceptos a partir de una colección de objetos y las relaciones que verifican. De esta forma, el propósito es poder representar y organizar la información de manera más cercana al pensamiento humano sin perder rigor científico. En este sentido se enmarca la cita de Rudolf Wille: ``\textit{El objetivo y el significado del FCA como teoría matemática sobre conceptos y sus jerarquías es apoyar la comunicación racional entre seres humanos mediante el desarrollo matemático de estructuras conceptuales apropiadas que se puedan manipular con la lógica.}''

El término FCA fue acuñado por Wille en 1984 culminando años más tarde con la publicación más citada al respecto en colaboración con Bernhard Ganter \cite{Ganter1997}. Desde entonces FCA se ha aplicado con éxito en diferentes disciplinas de la Ciencia, como por ejemplo: minería de datos, biología celular \cite{Endres2012}, genética \cite{Kaytoue2011}, economía, ingeniería del software \cite{Snelting1998}, medicina \cite{Motameny2008}, derecho \cite{Mimouni2015}, gestión de la información \cite{Priss2006}, etc.

La motivación principal de FCA aboga por representar conjuntos de objetos y atributos por medio de tablas de datos. Estas tablas se denominan contextos formales y representan las relaciones binarias entre esos objetos y atributos. Principalmente, existen dos formas básicas para representar el conocimiento: los retículos de conceptos y los conjuntos de implicaciones. 

Desde hace años, existen en la literatura estudios \cite{Kuznetsov2002} donde se han investigado y comparado diferentes algoritmos para obtener el retículo de conceptos a partir del conjunto de datos (en adelante \textit{dataset} por su nomenclatura habitual en el campo). Sin embargo, debido a que el tamaño del retículo de conceptos es, en el peor de los casos, $2^{min(|G|,|M|)}$, siendo $G$ el conjunto de objetos y $M$ el conjunto de atributos, el coste computacional de los métodos encargados de su construcción constituye una limitación de la aplicación de FCA.

Por otro lado tenemos el conjunto de implicaciones. Las implicaciones pueden considerarse \textit{grosso modo} como reglas del tipo \textit{si-entonces}, o en inglés: \textit{if-then rules}, cuya noción principal es un concepto muy intuitivo: cuando se verifica una premisa, entonces se cumple una conclusión. Esta idea básica se refleja en numerosos campos de conocimiento bajo diferentes nombres. Así, en base de datos relacionales se denominan dependencias funcionales \cite{Codd1970}, en FCA son implicaciones \cite{Ganter1997}, en programación lógica son reglas lógicas de programación \cite{Niemela1999}. No obstante, al igual que en el caso del retículo de conceptos, también existen ciertas desventajas a la hora de trabajar con implicaciones, de hecho, la propia extracción del conjunto completo de implicaciones de un dataset es una tarea que presenta una complejidad exponencial \cite{Cohen2001}. 

Trabajar con conjuntos de implicaciones nos permite utilizar técnicas automáticas basadas en la lógica, lo que nos conduce al propósito principal de esta tesis doctoral, que principalmente consiste en, utilizando los conjuntos de implicaciones, aplicar mecanismos lógicos para realizar un tratamiento eficiente de la información.

La aproximación a través de la lógica es posible gracias a sistemas axiomáticos válidos y completos como los Axiomas de Armstrong \cite{Armstrong74} y la Lógica de Simplificación \cite{Mora2004} (SL por sus siglas en inglés: \textit{Simplification Logic}). Estos métodos aplicados sobre conjuntos de implicaciones nos proporcionan la base para trabajar en esta tesis doctoral fundamentalmente sobre los siguientes tres campos de conocimiento: claves minimales, generadores minimales y sistemas de recomendación conversacionales.

En cada uno de los casos, vamos a aprovechar la información subyacente al conjunto de implicaciones, que son el núcleo principal de conocimiento en esta tesis, para realizar novedosas aproximaciones que nos permitan abordar problemas presentes en esos ámbitos. De esta forma, con los métodos desarrollados basados en implicaciones, hemos conseguido obtener resultados favorables para todos esos campos. Tales resultados se sustentan por una amplia gama de experimentos, en los cuales se ha utilizado tanto información real como conjuntos autogenerados y donde la computación paralela en entornos de supercomputación ha desempeñado un papel crucial.

Antes de entrar con mayor detalle en los anteriores tres campos, es necesario hacer una importante declaración previa. Tal y como se ha mencionado anteriormente, vamos a trabajar con el conjunto de implicaciones que se verifican en un dataset. Sin embargo, hay que dejar claro que no es competencia de este trabajo el estudiar técnicas para la extracción de estas implicaciones (lo cual es más una tarea de minería de datos), sino que la intención es partir del punto en el que ya contamos con el conjunto de implicaciones para trabajar con él. A este respecto, podemos mencionar los trabajos más citados en la literatura en relación a la extracción del conjunto de implicaciones a partir de datasets \cite{HuhtalaKPT99,YaoHB2002,Yevtushenko2006}. Son trabajos de suma importancia desde el punto de vista teórico pero también desde el punto de visto práctico, pues incluyen las implementaciones de las aplicaciones que realizan la extracción de las implicaciones. 
%Dicho esto, retomamos el texto pasando a introducir los tres campos de aplicación donde hemos utilizado los conjuntos de implicaciones.

%Sin perjuicio de lo anterior, dado que hemos llegado a crear datasets propios sobre los que poder realizar experimentos como veremos más adelante, si bien no entramos en las técnicas de extracción de implicaciones que utilizan esas aplicaciones, sí hemos tenido que convertirnos en usuarios de estas aplicaciones para poder obtener el conjunto atributos e implicaciones que se verifican. Dicho esto, retomamos el texto pasando a introducir los tres campos de aplicación donde hemos utilizado los conjuntos de implicaciones.

\section{Claves Minimales}
\noindent
El concepto de clave es fundamental en cualquier modelo de datos, incluyendo el modelo de datos relacional de Codd \cite{Codd1970}. Una clave de un esquema relacional está compuesta por un subconjunto de atributos que representan el \textit{dominio} de una determinada función cuya \textit{imagen} es la totalidad del conjunto de atributos. Estas funciones se pueden representar por medio de Dependencias Funcionales (FD, por sus siglas en inglés: \textit{Functional Dependencies}) que especifican una relación entre dos subconjuntos de atributos, e.g. $A$ y $B$, asegurando que para cualesquiera dos tuplas de una tabla de datos, si verifican $A$, entonces también verifican $B$. Nótese el cambio de denominación de implicación a dependencia funcional por estar en el entorno de los sistemas de base de datos relacionales. 

La identificación de las claves minimales de una determinada relación es una tarea crucial para muchas áreas de tratamiento de la información: modelos de datos \cite{Simsion2005}, optimización de consultas \cite{Kemper1991}, indexado \cite{Manolopoulos1999}, etc. El problema consiste en el descubrimiento de todos los subconjuntos de atributos que componen una clave minimal a partir de un conjunto de FD que se cumplen en un esquema de una tabla del modelo relacional.

Las claves no sólo son parte fundamental a considerar durante la fase de diseño de sistemas de base de datos relacionales, sino que también se consideran una poderosa herramienta para resolver multitud de problemas referentes a diversos aspectos del tratamiento de la información. Como muestras de la relevancia de este problema, encontramos numerosas citas en la literatura, entre las que podemos destacar las siguientes. En \cite{Sismanis2006}, los autores afirman que: ``\textit{identification of keys is a crucially important task in many areas of modern data management, including data modeling, query optimization (provide a query optimizer with new access paths that can lead to substantial speedups in query processing), indexing (allow the database administrator to improve the efficiency of data access via physical design techniques such as data partitioning or the creation of indexes and materialized views), anomaly detection, and data integration}''. Más recientemente, en referencia a áreas emergentes como el \textit{linked-data}, en \cite{Pernelle2013}, los autores delimitan el problema manifiestando: \textit{``establishing semantic links between data items can be really useful, since it allows crawlers, browsers and applications to combine information from different sources.''}

Para ilustrar de forma básica el concepto de clave, vamos a utilizar el siguiente Ejemplo \ref{ejemplo:basicoClaves}.

\begin{ejemplo}
\label{ejemplo:basicoClaves}
Supongamos que disponemos de la Tabla \ref{tabla:ejemploPeliculas}. Es una pequeña tabla donde se refleja información que relaciona títulos de películas, actores, países, directores, nacionalidad y años de estreno. 

De esta información, utilizando los métodos comentados anteriormente, podemos extraer el siguiente conjunto de FDs: 

$\Gamma = \{Titulo, A\tilde{n}o \rightarrow Pais$;  $Titulo, A\tilde{n}o \rightarrow Director$; $Director\rightarrow Nacionalidad$\}. 

\begin{table*}[htbp]
\caption{Tabla de películas}
\label{tabla:ejemploPeliculas}
\centering
{\scriptsize
\begin{tabular}{cccccc}
 \hline
 Título & Año & País & Director & Nacionalidad & Actor\\
 \hline
 Pulp Fiction & 1994 & USA & Quentin Tarantino & USA & John Travolta\\
 Pulp Fiction & 1994 & USA & Quentin Tarantino & USA & Uma Thurman\\
  Pulp Fiction & 1994 & USA & Quentin Tarantino & USA &Samuel Jackson \\
 King Kong & 2005 & NZ & Peter Jackson& NZ & Naomi Watts\\
 King Kong & 2005 & NZ & Peter Jackson & NZ & Jack Black\\
 King Kong & 1976 & USA & De Laurentiis & IT & Jessica Lange\\
 King Kong & 1976 & USA & De Laurentiis & IT & Jeff Bridges\\
 Django Unchained & 2012 & USA & Quentin Tarantino  & USA & Jamie Foxx\\
 Django Unchained & 2012 & USA & Quentin Tarantino  & USA & Samuel Jackson\\\hline
\end{tabular}
}
\end{table*}

Esta tabla tiene una sola clave minimal: $\{Titulo, A\tilde{n}o, Actor\}$ que corresponde con el conjunto de atributos necesario para identificar cualquier tupla de la relación.
\end{ejemplo}

Una característica muy importante de la claves es su minimalidad. Una clave se considerará minimal cuando todos y cada uno de los atributos que la forman son imprescindibles para mantener su naturaleza de clave, es decir, no contiene ningún atributo superfluo. %Formalmente:

%\begin{definicion}[Clave minimal]
%Dada un tabla $R$ sobre un conjunto de atributos $\Omega$, decimos que $K$ es una clave minimal de $R$ si se verifica la dependencia funcional $K\rightarrow \Omega$ en $R$ y $\not\exists a \in K$ tal que $K \smallsetminus \{a\} \rightarrow \Omega$.
%\end{definicion}



% El problema de la búsqueda de claves
\section*{El problema de la búsqueda de claves}
\label{sec:problemaBusquedaClaves}
% Problema de la búsqueda de claves
El problema de la búsqueda de claves consiste en encontrar todos los subconjuntos de atributos que componen una clave mínimal a partir de un conjunto de FD que se verifican en un esquema de una tabla de de datos relacional. Es un campo de estudio con décadas de antigüedad como vemos en \cite{Fadous75}, donde las claves se estudiaron dentro del ámbito de la matriz de implicaciones u otros tantos trabajos como \cite{Sali2004,Giannella99} que se centran en averiguar estas claves minimales.

% Claves como elemento crucial
La dificultad al enfrentarnos con el problema de la búsqueda de claves surge debido a que, dado un conjunto de atributos $A$, la cardinalidad del conjunto $2^A$ hace que haya que abordar el problema aplicando técnicas que guíen la búsqueda de los conjuntos candidatos a ser claves minimales. 

% problema NP
El cálculo de todas las claves minimales representa un problema complejo. En \cite{Lucchesi78,Yu76} se incluyen resultados interesantes acerca de la complejidad del problema; los autores demuestran que el número de claves minimales para un sistema relacional puede ser exponencial respecto al número de atributos, o factorial respecto al número de dependencias. Además, establecieron que el número de claves está limitado por el factorial del número de dependencias, por tanto, no existe un algoritmo que resuelva el problema en tiempo polinómico. En definitiva, es un problema NP-completo decidir si existe una clave de tamaño a lo sumo $k$ dado un conjunto de FD \cite{Lucchesi78}.

Es significativo como el problema de la búsqueda de claves aparece en diversos campos de conocimiento. Por ejemplo, en \cite{Benito-PicazoCMMSE2015} se hace mención a la importancia de conocer las claves en áreas emergentes como el \textit{linked-data}. Por otro lado, en \cite{CorderoEMG14}, mostramos cómo el problema de las claves mínimales en las bases de datos tiene su análogo en FCA, donde el papel de las FDs se manifiesta como implicaciones de atributos. En ese artículo, el problema de las claves mínimales se presentó desde un punto de vista lógico y para ello se empleó un sistema axiomático para gestionar las FDs y las implicaciones que los autores denominaron \slfde \cite{Enciso2002}.

% referencias generales
Las principales referencias sobre este problema apuntan a los trabajos de Lucchesi y Osborn en \cite{Lucchesi78} que muestran un algoritmo para calcular todas las claves candidatas. Por otro lado, Saiedian y Spencer \cite{Saiedian1996} presentaron un algoritmo usando grafos con atributos para encontrar todas las claves posibles de un esquema de base de datos relacional. No obstante, demostraron que sólo podía aplicarse cuando el grafo de FDs no estuviera fuertemente conectado. Otro ejemplo lo encontramos en el trabajo de Zhang \cite{Zhang09} en el cual se utilizan mapas de Karnaugh \cite{Karnaugh1953} para calcular todas las claves. Existen  más trabajos sobre el problema del cálculo de las claves minimales como son \cite{Sismanis2006,Worland2004} y otra contribución actual que aborda el problema en un estilo lógico \cite{CorderoEMG14}. Asimismo, en \cite{Levy2005,Valtchev03,Valtchev08} los autores propusieron el uso de FCA \cite{Ganter1997} para abordar problemas relacionados con la búsqueda y la gestión de las implicaciones, que pueden considerarse complementarios a nuestro trabajo.




\section*{Algoritmos para el cálculo de claves}
\label{sec:algoritmosCalculoClaves}
% referencias de tableaux
Por nuestra parte, como objetivo, nos hemos centrado en los algoritmos de búsqueda de claves basados en la lógica, y más específicamente, en aquellos que utilizan el paradigma de Tableaux \cite{Morgan1992,Risch1992} utilizando un sistema de inferencia. 

De forma muy general, podemos decir que los métodos tipo Tableaux representan el espacio de búsqueda como un árbol, donde sus hojas contienen las soluciones (claves). El proceso de construcción del árbol comienza con una raíz inicial y desde allí, las reglas de inferencia generan nuevas ramas etiquetadas con nodos que representan instancias más simples del nodo padre. La mayor ventaja de este proceso es su versatilidad, ya que el desarrollo de nuevos sistemas de inferencia nos permiten diseñar un nuevo método. Las comparaciones entre estos métodos se pueden realizar fácilmente ya que su eficiencia va de la mano con el tamaño del árbol generado.

Esto nos lleva a un punto de partida fundamental, los estudios de R. Wastl (Universidad de Wurzburg, Alemania) \cite{Wastl98a,Wastl98} donde se introduce por primera vez un sistema de inferencia de tipo Hilbert para averiguar todas las claves de un esquema relacional. A modo de ejemplo básico, en la Figura \ref{figura:ejemploTaleaux} podemos ver un ejemplo de árbol de búsqueda según el paradigma de Tableaux desarrollado según las reglas de inferencia del sistema de inferencia $\mathbb{K}$ de Wastl.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics*[width=.75\textwidth,height=.3\textheight]{arbol897.png}
	\end{center}
	\caption{Ejemplo de Tableaux utilizando el sistema de inferencia $\mathbb{K}$ de Wastl.}
	\label{figura:ejemploTaleaux}
\end{figure}

Siguiendo esta línea, en \cite{Cordero2013} los autores abordan el problema de la búsqueda de claves utilizando un sistema de inferencia basado en la lógica de simplificación para dependencias funcionales \cite{Enciso2002} demostrando como el árbol del espacio de búsqueda que se genera nos lleva a sobrepasar las capacidades de la máquina, incluso para problemas pequeños. En \cite{Mora2012} los autores muestran la equivalencia entre \slfde y los axiomas de Armstrong \cite{Armstrong74} junto con un algoritmo para calcular el cierre de un conjunto de atributos. Más tarde, en \cite{CorderoEMG14}, los autores introdujeron el método SST, basado en la introducción del test de minimalidad que evita la apertura de ramas adicionales del árbol, por lo que el espacio de búsqueda se vuelve más reducido, logrando un gran rendimiento en comparación con sus predecesores.

% con el paralelo
Una propiedad muy interesante de los métodos basados en Tableaux es su generación de subproblemas independientes los unos de los otros a partir del problema original. De esta forma llegamos a nuestro objetivo fundamental de utilizar las técnicas lógicas sobre una implementación paralela de los métodos que, mediante el uso de recursos de supercomputación, nos permitan alcanzar resultados en un tiempo razonable.

En esta línea, son varios los trabajos que han utilizado la paralelización para afrontar problemas relacionados con implicaciones o FCA. Un algoritmo paralelo para el tratamiento de implicaciones enmarcado en el campo de los hipergrafos lo podemos encontrar en \cite{Sridhar1990}. A su vez, Krajca et al. \cite{Krajca2008} presentan un algoritmo paralelo para el cálculo de conceptos formales. Por nuestra parte, una primera aproximación a la paralelización del método de Wastl \cite{Wastl98a,Wastl98} y el algoritmo de claves \cite{Cordero2013} fue presentado en \cite{Benito-Picazo2014}, donde se muestra cómo el paralelismo puede integrarse de forma natural en los métodos basados en Tableaux.

Para la labor de computación de alto rendimiento, a lo largo de este trabajo se han adquirido y aplicado las competencias necesarias para poder trabajar en entornos de supercomputación; en concreto, durante los últimos años, se ha trabajado fervientemente con el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga\footnote{http://www.scbi.uma.es/}. La posibilidad de tratar con este Centro nos ha proporcionado dos beneficios fundamentales: por un lado, se ha alcanzado una elevada pericia para trabajar en entornos de HPC y para realizar implementaciones que aprovechen una alta cantidad de recursos, y por otro lado, se han podido obtener resultados empíricos sobre experimentos utilizando estrategias de paralelismo, que han desembocado en contribuciones científicas \cite{Benito-Picazo2014,Benito-Picazo2016} y que habría sido imposible conseguir en la actualidad sin contar con tales recursos computacionales.


\section*{Métodos \textit{SST} y \textit{CK}}
\label{sec:metodosSSTyCK}
En \cite{CorderoEMG14} se presentó un nuevo algoritmo, denominado SST, para calcular todas las claves minimales usando una estrategia de estilo Tableaux, abriendo la puerta a incorporar el paralelismo en su implementación. SST se basa en la noción de cierre de conjunto, una noción básica en la teoría de base de datos que permite caracterizar el conjunto máximo de atributos que se puede alcanzar, desde un determinado conjunto de atributos $A$ con respecto a un conjunto de FD, utilizando el sistema axiomático. Por lo tanto, si el cierre de $A$ se denota como $A^+_\Gamma$, el sistema de inferencia para FD nos permite inferir la FD $A\to A^+_\Gamma$. El enfoque con estilo lógico para el problema de las claves minimales consiste en la enumeración de todos los conjuntos de atributos $A$ tales que se verifique la FD: $A\to\Omega$.

SST muestra un gran rendimiento en comparación con sus predecesores como hemos visto hasta ahora y como puede comprobarse en el amplio estudio realizado sobre el método en \cite{Benito-Picazo2014TFM}. El beneficio principal en la reducción del espacio de búsqueda de debe a la introducción del test de inclusión para evitar la apertura de ramas extra. Gracias a ello, SST no abre algunas ramas que sabemos que van a producir las mismas claves que se calculan en otra rama.

Basándonos en el sistema axiómatico de la lógica \slfde \ref{sec:logicaSimplificacion}, proponemos un nuevo método llamado \textit{Closure Keys (CK)} que incorpora un mecanismo eficiente de poda que utiliza el método de cierre basado en \slfde para mejorar el método SST. El nuevo operador de cierre definido en \cite{Mora2012} permite reducir el espacio de búsqueda realizando reducciones en el camino hacia las hojas, donde finalmente se obtienen las claves.  El método CK tiene una característica fundamental que lo convierte en un novedoso enfoque a los métodos clásicos de cierre, ya que proporciona una nueva funcionalidad: además del conjunto de atributos que constituye la salida del operador de cierre, el método proporciona un subconjunto de implicaciones del conjunto $\Gamma$ original.

Por tanto, el método CK recibe un conjunto de implicaciones $\Gamma$ y un subconjunto de atributos $X \subseteq \Omega$, calcula el conjunto cierre $X^+$ respecto a $\Gamma$, y además, un nuevo conjunto $\Gamma^\prime$ que contiene el conjunto de implicaciones que guarda la semántica que queda fuera del cierre $X^+$. Si $\Gamma^\prime = \varnothing$, entonces $X^+ = \Omega$ (véase \cite{Mora2012} para más detalles).

Finalmente, la principal contribución de esta parte de la tesis se produce de la siguiente forma. Se ha desarrollado una implementación paralela tanto del método SST como del método CK basándose en el paradigma \textit{MapReduce} \cite{Dean2004}. Básicamente, el algoritmo paralelo de búsqueda de claves se divide en dos partes principales. Utiliza una primera fase en la que se realiza una expansión del árbol de búsqueda trabajando sobre el problema original pero llegando únicamente hasta un cierto nivel de árbol, es decir sin alcanzar todavía las claves en las hojas del árbol. En ese momento interviene la segunda etapa en la que cada nodo de ese nivel del árbol, que constituye un problema equivalente al original pero simplificado como resultado de la aplicación de las reglas de inferencia hasta ese momento, se resuelve en paralelo mediante el uso de un elevado número de cores, es decir, aplica el algoritmo de búsqueda de claves, pero ahora sí, hasta alcanzar las hojas del árbol.

De esta forma, se han realizado multitud de experimentos para confirmar las mejoras que se han alcanzado, los cuales necesitan llevarse a cabo en entornos de supercomputación y cuyos resultados pueden consultarse en \cite{Benito-Picazo2016}. Principalmente, se ha demostrado como estos métodos son claramente adecuados para ejecutarse utilizando una implementación paralela. De esta forma, se consiguen resultados en tiempos razonables incluso cuando la cantidad de información de entrada es considerable y donde los métodos secuenciales no son capaces de finalizar.


\section{Generadores Minimales}
Como se ha mencionado anteriormente, una forma de representar en FCA el conocimiento es el retículo de conceptos. Esta representación otorga una visión global de la información con un formalismo muy fuerte, abriendo el puerta para utilizar la teoría de retículos como una metateoría para gestionar la información \cite{Bertet2016}.

Los conjuntos cerrados son la base para la generación del retículo de conceptos ya que éste puede ser construido a partir de los conjuntos cerrados, considerando la relación de subconjuntos como la relación de orden. En este punto nace el concepto de generadores minimales como aquellas representaciones de los conjuntos cerrados que no contengan información supérflua, es decir, representaciones canónicas de cada conjunto cerrado \cite{Ganter1997}.

Los generadores minimales junto con los conjuntos cerrados son esenciales para obtener una representación completa del conocimiento en FCA, pero no sólo son interesantes desde un punto de visto teórico. La importancia de los generadores minimales puede apreciarse claramente a través de citas tales como: \textit{``Minimal generators have some nice properties: they have relatively small size and they form an order ideal''}, existente en importantes estudios como el de Poelmans et al. \cite{Poelmans2013} o \cite{Qu2007}. Además, los generadores minimales se han usado como punto clave para generar bases, las cuales constituyen una representación compacta del conocimiento que facilita un mejor rendimiento de los métodos de razonamiento basados en reglas. Missaoui et al. \cite{Missaoui2010,Missaoui2012} presentan el uso de generadores minimales para calcular bases que impliquen atributos positivos y negativos cuyas premisas son generadores minimales.

Cualquier operador de cierre $c$ en $M$ puede asociarse con un sistema de implicaciones. Esta conexión establece una forma de gestionar el trabajo del operador de cierre $c$ por medio de su derivación sintáctica y, como consecuencia, se puede elaborar un método para realizar esta gestión. Pero, ¿qué pasa con la conexión inversa? Es decir, dado un conjunto de implicaciones, ¿es posible generar el operador de cierre $c$ asociado a él? Tal pregunta es el núcleo de esta parte de la tesis y su solución implica enumerar todos los conjuntos cerrados.

Si tenemos que $X,Y \subseteq M$ satisfacen que $X = Y^+_\Sigma$, es habitual decir que $Y$ es un generador del conjunto cerrado $X$. Obsérvese que cualquier subconjunto de $X$ que contiene $Y$ es también un generador de $X$. Dado que trabajamos con conjuntos finitos de atributos, el conjunto de los generadores de un conjunto cerrado se pueden caracterizar por sus generadores minimales.


\section*{Métodos para el cálculo de generadores minimales}
\label{seccion:metodosGeneradoresMinimales}
En nuestro caso, centrándonos una vez más en el tratamiento inteligente de los conjuntos de implicaciones, vamos a utilizarlos como los elementos para describir la información y además, como base del diseño de métodos para enumerar todos los conjuntos cerrados y sus generadores minimales a partir de esta información, y no del \textit{dataset} original (como en los trabajos mencionados), lo cual, hasta donde sabemos, no existe trabajo previo al respecto. 

El nuevo método propuesto es una evolución del método presentado en \cite{Cordero2012} donde se utilizó la \slfde como herramienta para encontrar todos los generadores minimales a partir de un conjunto de implicaciones. Este método trabaja sobre el conjunto de implicaciones aplicando unas reglas de inferencia y construyendo un espacio de búsqueda de árbol muy parecido a los árboles del caso de las claves minimales.

Concretamente, dado un conjunto de atributos $M$ y un sistema de implicaciones $\Sigma$, el método realiza un mapeo $mg_\Sigma\colon 2^M\to 2^{2^M}$ que satisface la siguiente condición.

$$\forall X,Y\subseteq M$$

\begin{center}
$X\in mg_\Sigma(C)$ si y sólo si $C$ es cerrado para $(\ )^+_\Sigma$ y $X$ es un generador minimal para $C$.
\end{center}

\begin{ejemplo}
Sea $\Sigma=\{a\to c, bc\to d, c\to ae, d\to e\}$, el mapeo $mg_\Sigma$ se describe como:
\begin{center}
\begin{tabular}{p{1.6cm}|p{.4cm}p{.3cm}p{.3cm}p{.5cm}p{.5cm}p{.6cm}p{.6cm}p{.8cm}p{.9cm}}
$X$ & $\varnothing$ & $b$ & $e$ & $be$ & $de$ & $ace$ & $bde$ &  $acde$ &   $abcde$   \\
\hline
$mg_\Sigma(X)$ &$\varnothing$ & $b$ & $e$ & $be$ & $d$  & $a$ & $bd$ & $ad$ & $ab$ 
\\
& & & & & & $c$& &$cd$ & $bc$    
\end{tabular}
\end{center}

En otro caso, $X$ no es cerrado y $mg_\Sigma(X)=\varnothing$. Nótese que $\varnothing$ es cerrado y $mg_{\Sigma}(\varnothing)=\{\varnothing\}$, i.e. $\varnothing$ es un generador minimal del conjunto cerrado $\varnothing$.
\end{ejemplo}

Tras el método presentado en \cite{Cordero2012} (que denominaron MinGen) se presenta ahora un nuevo método (MinGenPr) que aplica una importante mejora con respecto al anterior. Básicamente consiste en incorporar un mecanismo de poda, basada en el test de inclusión de conjuntos que involucra a todos los nodos del mismo nivel, para evitar la generación de generadores minimales y cierres redundantes. El propósito de esta poda es verificar la información de cada nodo en el espacio de búsqueda, evitando la apertura de una rama completa. Finalmente, se propone un último método (GenMinGen) que generaliza la estrategia de poda anterior al considerar el test de inclusión del subconjunto no sólo con la información de los nodos del mismo nivel, sino también con todos los generadores minimales calculados antes de la apertura de cada rama.

En definitiva, se han estudiado e implementado cada uno de estos métodos en su versión secuencial, y para evaluar el rendimiento e ilustrar las mejoras obtenidas al pasar de un método a otro, se han realizado un gran número de pruebas utilizando información autogenerada e información real.



\section*{Generadores minimales y paralelismo}
\label{seccion:generadoresMinimalesParalelismo}
La contrapartida es que la obtención de todos los conjuntos cerrados y sus respectivos generadores minimales es un problema con complejidad exponencial. Sin embargo, dado que nuestro objetivo es explotar las posibilidades de operar con conjuntos de implicaciones, en este trabajo vamos a combinar ambos aspectos enumerando todos los conjuntos cerrados a partir de un conjunto dado de implicaciones. De hecho, iremos un paso más allá, calculando no sólo todos los conjuntos cerrados, sino que para cada uno de ellos produciremos sus respectivos generadores minimales.

No obstante, vamos a encontrarnos con un problema similar al que sucedía con las claves minimales, y es que, al tratar con grandes cantidades de información, los métodos realizados para producir los generadores minimales, conllevan unas necesidades de cómputo que sobrepasan los límites de la máquina. Por tanto, una vez más, tenemos que trasladar las implementaciones de los métodos a versiones paralelas que nos permitan funcionar bajo arquitecturas de supercomputación. 

En este sentido, se han realizado tanto las implementaciones secuenciales como paralelas de los métodos de producción de generadores minimales (MinGen, MinGenPr, GenMinGen, MinGenPar). Para las versiones paralelas vamos a utilizar la misma filosofía de implementación que en el caso de las claves minimales, es decir, hemos desarrollado unos códigos, basándonos en el esquema \textit{MapReduce} \cite{Dean2004}, que se ejecutarán en dos etapas. Al igual que en las claves minimales, la primera etapa divide el problema de entrada en varios subproblemas equivalentes pero reducidos de forma que puedan ser tratados de forma independiente. En la segunda etapa, cada uno de estos subproblemas se resuelve en paralelo usando múltiples núcleos.

Una vez más, para verificar el rendimiento y la idoneidad de los métodos para aplicar estrategias paralelas, se ha realizado una amplia batería de pruebas tanto sobre información autogenerada como información real. Además, las pruebas han incluido tareas de estimación del número óptimo de cores a utilizar así como del valor de corte más apropiado en la etapa primera de los métodos paralelos. Los resultados obtenidos al respecto pueden consultarse en \cite{Benito-PicazoCMMSE2017}. 




\section{Sistemas de Recomendación Conversacionales}
\noindent
La última aplicación que se ha llevado a cabo en esta tesis doctoral haciendo uso de los conjuntos de implicaciones y los conjuntos cerrados se produce en el campo de los sistemas de recomendación (SR). 

%El objetivo principal de los SR es ayudar al usuario a elegir entre un número alto de alternativas.  

De forma muy básica, podríamos considerar que un SR es un sistema inteligente que proporciona a los usuarios una serie de sugerencias personalizadas (recomendaciones) sobre un determinado tipo de elementos (ítems). De forma general, los SR estudian las características de cada usuario e ítem del sistema, y mediante un procesamiento de los datos, encuentra un subconjunto de ítems que pueden resultar de interés para el usuario. Una de las referencias más notables en el campo de los SR la encontramos en el libro de Adomavicius y otros \cite{AdomaviciusBook11}.

% Historia
Desde que el primer SR hizo su aparición en el mundo de las tecnologías de la información \cite{Hill1995,Resnick1997}, los SR han estado en continua evolución durante los últimos años \cite{Adomavicius2005}. Sin embargo, es con la expansión de las nuevas tecnologías cuando han tenido un acercamiento más directo a la mayor parte de la sociedad debido a su capacidad para realizar todo tipo de recomendaciones sobre diversos elementos al alcance de todos (libros \cite{Crespo2011}, documentos \cite{Porcel2012}, música \cite{LampropoulosLT12}, turismo \cite{BorrasFPMVIORC11}, películas\footnote{https://www.movielens.org}, etc.).

% Importancia
En la actualidad, los SR constituyen un claro campo de investigación y estudio como demuestran el gran número de trabajos que se están realizando \cite{Son2018,Eirinaki2018} y cuya cantidad continúa aumentando día a día. Además, la relevancia de estos sistemas no se limita al ámbito investigador. Actualmente, muchos SR ya han sido implantados con éxito en fuertes entornos comerciales a nivel mundial. Este es el caso de empresas líderes en el sector como pueden ser Amazon \cite{linden2003}, LinkedIn \cite{Metaphor2012} o Facebook \cite{Tiroshi11}, que han realizado fuertes inversiones con el fin de generar mejores SR. Estas situaciones ponen de manifiesto la gran importancia de estos sistemas en ambas vertientes de la sociedad actual.

% FCA y SR
Abordar la generación de recomendaciones haciendo uso de FCA es una aproximación existente en la literatura desde hace años. En \cite{duBoucherRyan2006}, los autores utilizan FCA para agrupar elementos y usuarios en conceptos para posteriormente, realizar recomendaciones colaborativas según la afinidad con los elementos vecinos. Más tarde, en \cite{Senatore2013}, los autores introducen un modelo para el filtrado colaborativo basado en FCA para generar correlaciones entre datos a través de un diseño del retículo. Zhang et al. \cite{Zhang2015} propusieron un sistema basado en similitud agrupando la información contextual en grafos mediante el cual llevar a cabo recomendaciones sobre las interacciones sociales entre usuarios. En \cite{LeivaERCMG13,LeivaERCMG13a}, se utilizan relaciones difusas e implicaciones ponderadas para especificar el contexto y \slfde para desarrollar un proceso lineal de filtrado que permite a los SR podar el conjunto original de elementos y así mejorar su eficiencia. Recientemente, en \cite{Zou2017} se propone y utiliza un novedoso SR personalizado basado en el retículo de conceptos para descubrir información valiosa de acuerdo con los requisitos e intereses de los usuarios de forma rápida y eficiente. Todos estos trabajos subrayan claramente cómo FCA puede aplicarse con éxito en el campo de los SR.


\section*{Técnicas de recomendación}
\label{seccion:tecnicasRecomendacion}
% Tipos
Existen numerosos tipos diferentes de SR que normalmente se clasifican atendiendo a cómo se llevan a cabo las recomendaciones. Los más conocidos y extendidos son los sistemas de filtrado colaborativo (denominados CF, por sus siglas en inglés: \textit{Collaborative Filtering}) basan su funcionamiento fundamentalmente en las valoraciones que otros usuarios han otorgado a los elementos disponibles; y los sistemas basados en contenido (denominados CB, por sus siglas en inglés: \textit{Content-Based}) que se basan en categorizar los ítems a recomendar, proporcionando resultados que tengan características similares a otros que han sido bien valorados anteriormente por el usuario.

En los últimos años ha habido un gran crecimiento de los SR contextuales \cite{BenSassi2017}, capaces de tener en cuenta información relevante para la recomendación como puede ser la hora, el lugar, la compañía, la ubicación, etc. Los SR demográficos \cite{BeelLNG13} que clasifican a los usuarios según diferentes parámetros personales (edad, localización, etc.). Por otro lado encontramos los denominados SR basados en conocimiento (KB, por sus siglas en inglés: \textit{Knowledge-Based}) \cite{Mandl2011}. Estos sistemas gestionan el conocimiento inherente a los datos y revelan cómo un ítem puede satisfacer la necesidad del usuario, es decir, utilizan un método de razonamiento para inferir la relación entre una necesidad y una posible recomendación. Finalmente, llegamos a los SR más importantes desde el punto de vista de este trabajo, los denominados SR conversacionales \cite{Griol2018,Lee2017}. Estos SR están estrechamente relacionados con los conceptos de recomendador basado en críticas \cite{Chen2012} y recomendaciones de información \cite{TrabelsiWBR11}. Resaltamos este tipo de SR porque será la estrategia principal sobre el que se sustenta el desarrollo de SR realizado y que ha dado lugar a una de las contribuciones que avalan esta tesis \cite{Benito-Picazo2017}. Se puede consultar una clasificación más detallada en el libro de Adomavicius y Tuzhilin \cite{AdomaviciusBook11} que, junto con la contribución de Bobadilla et al. \cite{Bobadilla2013}, constituyen las referencias más citadas en el campo.

Podemos apreciar que existen diferentes tipos de estrategias para los SR, sin embargo, la historia ha demostrado ampliamente que la mejor alternativa consiste en combinar características de diferentes tipos de SR para generar híbridos que se beneficien de las ventajas de cada uno de ellos \cite{DeCampos2010}. Tal es nuestro caso, en el que nuestro trabajo ha culminado en un SR híbrido que combina las siguientes técnicas de recomendación:

\begin{itemize}
	\item \textbf{SR basados en conocimiento.} En virtud de nuestro estudio de extracción de conocimiento de los datos utilizando FCA, los conjuntos de implicaciones y los operadores de cierre.
	\item \textbf{SR basados en contenido.} Necesario ya que para aplicar las técnicas lógicas empleadas necesitamos en primera instancia información sobre la que trabajar. 
	\item \textbf{SR conversacionales.} Se presenta como estrategia central sobre la que aplicar y utilizar las dos anteriores respectivamente.
\end{itemize}

Pero no nos ocuparemos únicamente de la estrategia de recomendación sino también del proceso de cómo obtener una recomendación. En este sentido, se introduce el concepto de Recuperación de Información (IR por sus siglas en inglés, \textit{Information Retrieval}), que básicamente se encarga de realizar búsquedas para determinar cuán bien responde cada objeto a una consulta. Numerosos trabajos en la literatura relacionan el uso de FCA en modelos basados en IR \cite{Codocedo2015,Ignatov2015}.


\section*{Problemas comunes}
\label{seccion:problemasRecomendacion}
Si bien es cierto que los SR están alcanzando una enorme importancia existen numerosas dificultades que han de afrontarse a la hora de diseñar e implementar un SR. En la lista de problemas relacionados con los SR \cite{Shah2016} podemos encontrar: el arranque en frío \cite{Feil2016,Son201687}, privacidad \cite{Friedman2015}, oveja-negra \cite{Gras2016}, escasez \cite{Guo2012}, ataques maliciosos \cite{Zhou2015,Yang2016}, sobreespecialización \cite{LopsGS11}, escalabilidad \cite{Isinkaye2015}, postergación \cite{Sundaresan2011}, dimensionalidad \cite{Salimi2017}, etc.

En concreto, nuestro trabajo ha estado orientado a abordar este último problema de la dimensionalidad en los SR. Este problema, también conocido como \textit{the curse of dimensionality phenomenon} \cite{Salimi2017,Nagler2016} aparece cuando es necesario trabajar sobre datasets con un alto número de características (variables o atributos). De forma intuitiva, podríamos introducirlo de la siguiente manera. Cuando hay pocas columnas de datos, es relativamente fácil para los algoritmos realizar tareas de tratamiento inteligente de la información como: aprendizaje automático, \textit{clustering}, clasificación, etc. Sin embargo, a medida que aumentan las columnas o características de nuestros ítems, se vuelve exponencialmente más difícil hacer labores predictivas con un buen nivel de precisión. El número de filas de datos necesarias para realizar cualquier modelado útil aumenta exponencialmente a medida que agregamos más columnas a una tabla.

Para abordar este problema, podemos encontrar muchos trabajos en la literatura sobre la reducción de la dimensión de la información, especialmente mediante selección de características \textit{(feature selection)}, que pueden ayudarnos a descartar aquellas características que no son merecedoras de ser considerados según diferentes criterios. De hecho, estas técnicas ya se aplican en otras áreas como son: algoritmos genéticos o redes neuronales, normalmente centrándose en la aplicación de un proceso automatizado que se aplique de una vez \textit{(batch mode)} mediante selección de características.

%Suele ser habitual que para realizar una selección de características por parte del SR, el usuario tenga que introducir y seleccionar información del sistema una y otra vez. Esto constituye un problema dado que pueden existir artículos para los cuales el número de características que los definen sea muy elevado y en consecuencia, incomode la correcta interacción del usuario con el sistema, poniendo de manifiesto de nuevo cómo la alta dimensionalidad constituye un problema para los SR.

En definitiva, nuestro objetivo ha sido abordar el problema de la alta dimensionalidad en los SR haciendo uso de los conjuntos de implicaciones a través de un proceso de selección de atributos por parte del usuario mediante un SR conversacional que utilice características de los SR basados en contenido y en conocimiento.

Un trabajo interesante en esta área es \cite{Jannach2009}, que establece la idoneidad de los enfoques basados en el conocimiento para los procesos conversacionales.
En particular, estos autores utilizan el razonamiento basado en restricciones, en lugar de nuestro enfoque basado en la lógica. Además, este trabajo trata sobre
concepto de optimización de consultas, análogo al aplicado en nuestra propuesta. Otro trabajo notable es \cite{TrabelsiWBR11}, que comparte nuestro objetivo de disminuir el número de pasos de la conversación. Los autores proponen métricas acerca del número de pasos de la conversación y tasas de poda, ambos muy similares a los utilizados en nuestro trabajo. Por otro lado, en \cite{Chen2007}, los autores demuestran cómo la posibilidad de que sea el usuario el encargado de la selección de atributos supera al hecho de que sea el sistema mismo el encargado de dicha selección. Este hecho respalda nuestro enfoque en el cual el humano experto guía la conversación y el proceso de selección de características.


\section*{Evaluación de los sistemas de recomendación}
\label{seccion:evaluacionSistemasRecomendacion}
La evaluación de las predicciones y recomendaciones se ha convertido en un aspecto muy importante \cite{Herlocker2004,Burke2010}. Los SR requieren medidas de calidad y métricas de evaluación \cite{Gunawardana2009} para conocer la calidad de las técnicas, métodos y algoritmos para las predicciones y recomendaciones. Las métricas de evaluación \cite{HernandezdelOlmo2008} y los \textit{frameworks} de evaluación \cite{Bobadilla2011} facilitan la comparación de varias soluciones para el mismo problema.

No obstante, hay que tener en cuenta que dependiendo del SR con el que estemos trabajando, la evaluación habrá que llevarla a cabo utilizando aquellas métricas, que por su naturaleza y significado, tengan cabida en relación al SR que se desea evaluar. En nuestro caso, dado que hemos desarrollado un SR conversacional, puede ser evidente que la primera medida que podemos aplicar es calcular el número de pasos que se producen en la conversación \cite{McSherry01}. Por contra, otras métricas tan populares como son \textit{Precision} y \textit{Recall} \cite{Gunawardana2015} no son adecuadas de aplicar.


\section*{Aplicación desarrollada}
\label{seccion:aplicacionDesarrollada}
La solución que hemos realizado consiste en gestionar el problema de la alta dimensionalidad mediante un proceso de selección de características guiado por el usuario (experto humano) dentro de un sistema conversacional \cite{Yu2016}. Para ello, una vez más haremos uso de una gestión inteligente de las implicaciones y de los conjuntos cerrados que nos va a permitir reducir sustancialmente el número de pasos necesarios en el diálogo entre el usuario y la aplicación para conseguir una recomendación adecuada en tiempo y forma. Para ello contaremos con el apoyo de la \slfde introducida por os autores en \cite{Enciso2002} y en particular, se utilizará el algoritmo del cierre de atributos \slfde \cite{Mora2012} como núcleo de un marco de selección de atributos que será el que nos permita reducir el número de etapas del diálogo.

Además, se han realizado numerosas pruebas de aplicación sobre la propuesta que se ha desarrollado. En concreto, se han realizado pruebas utilizando información real sobre enfermedades y fenotipos como podemos apreciar en una de las contribuciones que avalan este trabajo de investigación \cite{Benito-Picazo2017}. Además, nuestra propuesta, al igual que la gran mayoría de los SR, permite actuar sobre diferentes \textit{datasets} de forma que podemos utilizar el mismo procedimiento para mejorar las recomendaciones conversacionales en diversos entornos. Esta versatilidad es una característica notoria, ya que permite libertad de maniobra en el caso de que se quieran introducir ciertos cambios, o simplemente que los datos sean diferentes. En ese sentido, la propuesta de este trabajo casa con los conceptos de adaptabilidad y longevidad de los SR ya que el funcionamiento es independiente de la información de base con la que trabaje, sólo necesitamos conocer el conjunto de atributos e implicaciones subyacente a los datos.

Finalmente, antes de llegar a las últimas líneas del capítulo que dedicaremos a establecer el contenido del documento y las contribuciones producidas, cabe resaltar ya en este punto la naturaleza dual de la tesis, en el sentido de que mantendremos una línea de investigación en fundamentos teóricos complementada con la aplicación de dichos resultados en los dos campos de conocimiento mencionados anteriormente, bases de datos y sistemas de recomendación. Haremos especial hincapié en la parte aplicada del estudio con la intención de facilitar la transferencia de conocimiento a entornos diferentes del ámbito académico como el mercado empresarial. Pasamos entonces ahora a desglosar la estructura del documento y la producción conseguida.

%\vspace{2cm}
\newpage{}
\subsection*{Estructura de la Tesis}
\noindent
En este primer capítulo de introducción, hemos fijado los puntos fundamentales de la tesis, como son: el marco de trabajo sobre el que vamos a actuar, las técnicas que utilizaremos y los principales objetivos que se pretenden alcanzar. Concretamente, hemos estipulado la utilización de FCA y los conjuntos de implicaciones como base del estudio sobre la que aplicar técnicas basadas en la lógica para mejorar el tratamiento de la información.

A continuación, entraremos en el Capítulo \ref{cap:preliminares}, en el que hemos recopilado un conjunto de conceptos previos necesarios relacionados con: FCA, la lógica de simplificación, los sistemas de implicaciones y los operadores de cierre. Tras estos dos primeros capítulos, ya contaremos con el conocimiento previo necesario para abordar las dos partes principales de la tesis. 

Tras la introducción y los preliminares, pasamos a un tercer capítulo \ref{cap:clavesMinimales} en el que se presenta la primera contribución que avala este trabajo de investigación y que corresponde con el trabajo realizado en el campo de la búsqueda de las claves minimales. De forma general, este artículo presenta nuevos métodos para resolver el problema de la inferencia de claves minimales en esquema de datos basándose en la \slfde y el uso de implicaciones. Además, refleja las implementaciones y las ventajas obtenidas al aplicar técnicas de computación paralela para poder aplicar los métodos sobre conjuntos de datos de una entidad tal que técnicas secuenciales no son capaces de gestionar en cuanto a tiempo y recursos necesarios. Para ello se presentan los resultados obtenidos para los experimentos en entornos de supercomputación.

A continuación, llegamos a un capítulo análogo al anterior pero ahora para el tema de estudio referente a los generadores minimales \ref{cap:clavesMinimales}. Este capítulo presenta un nuevo artículo en el cual se lleva a cabo un estudio de los métodos de producción de generadores minimales basados en la lógica y el tratamiento de implicaciones. Se comprueba las mejoras de rendimiento de los métodos al aplicar reducciones en el espacio de búsqueda basadas en estrategias de poda. Al igual que en el caso de las claves minimales, se presentan las implementaciones paralelas de los métodos para poder tratar con conjuntos de datos de tamaño considerable y se incluyen las pruebas realizadas en entornos de supercomputación.

Como último capítulo dedicado a las aplicaciones desarrolladas mediante la gestión de implicaciones se presenta el capítulo \ref{cap:sistemasRecomendacion}. En este capítulo se incluye un novedoso artículo en el que se desarrolla una aproximación al tratamiento del problema de la dimensionalidad que aparece en el campo de los sistemas de recomendación. En él, mediante el uso eficiente de la \slfd, las implicaciones y los operadores de cierre, se consigue un modelo de sistema de recomendación conversacional que es capaz de gestionar el problema de la dimensionalidad aliviando la sobrecarga de información con la que el usuario debe lidiar a la hora de obtener una recomendación por medio de un sistema conversacional. Asimismo, se demuestra su utilidad en entornos reales mediante la realización de un experimento sobre información real.

Finalmente, cerraremos la tesis con un último capítulo \ref{cap:conclusiones} dedicado a recopilar las principales conclusiones obtenidas y a proponer caminos por los que seguir ahondando en la investigación en esta materia. Además, se incluye una relación de las referencias consultadas y los respectivos índices de figuras, términos y tablas.

En aras de la completitud, se incluyen como anexos finales aquellos artículos que han sido publicados a lo largo de este periodo de investigación, que si bien no se utilizan como respaldo para esta tesis doctoral por no corresponder con los criterios de calidad exigidos a tal efecto, han sido la semilla y experiencia inicial para conseguir el resto de trabajos que sí actúan como aval.

En la Figura \ref{figura:esquemaTesis} se muestra de forma gráfica el contenido de la tesis y las contribuciones.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics*[width=1\textwidth,height=.8\textheight]{esquemaTesis.png}
	\end{center}
	\caption{Esquema de la estructura de la tesis y las publicaciones.}
	\label{figura:esquemaTesis}
\end{figure}
