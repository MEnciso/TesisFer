\pagestyle{empty}
\chapter{Introducción}\label{cap:introduccion}
%\addcontentsline{toc}{chapter}{\protect{Introduction}}

%\markboth{Introduction}{Introduction}

\pagestyle{headings}

\bigdrop{0pt}{5}{cmr10}La gestión de la información es uno de los pilares esenciales de la Ingeniería Informática. No es de extrañar, por tanto, que conforme un amplio campo de investigación y conocimiento donde diversas disciplinas como las Matemáticas, la Lógica y la \crerror*[inline]{Ingenería Informática (¿o Computación?)}{\textst{Ingeniería}} actúen conjuntamente para alcanzar mejores sinergias.

Dentro de \crerror*[inline]{este ámbito}{\textst{esta filosofía}} y \crerror*[inline]{con la intención de hacer aportaciones en campos de la Ingeniería Informática }{\textst{en aras de mejorar el funcionamiento de dos tipos de sistemas de información}} como son las bases de datos y los sistemas de recomendación, \crerror*[inline]{esta tesis doctoral toma como principal base teórica}{\textst{en esta tesis doctoral nos vamos a centrar en}} el Análisis \crerror*[inline]{de Conceptos Formales}{\textst{Formal de Conceptos}} (FCA, por sus siglas en inglés: \textit{Formal Concept Analysis})\crerror*[inline]{y, más concretamente, }{\textst{; concretamente, en la utilización de}} dos de sus \crerror*[inline]{herramientas}{\textst{conceptos}} fundamentales: los retículos de conceptos y los sistemas de implicaciones. La gestión inteligente de estos elementos mediante técnicas lógicas y computacionales confieren una alternativa para superar obstáculos \crerror*[inline]{en los campos mencionados.}{\textst{que podemos encontrar a la hora de trabajar con los sistemas de información mencionados. Comenzamos pues introduciendo brevemente FCA.}}
\begin{acrerror*}
{No me gusta mucho que la tesis esté escrita en primera persona...}{}
\end{acrerror*}
\crerror*[inline]{}{\textst{Podemos considerar }}FCA \crerror*[inline]{es}{\textst{como}} una teoría matemática y una metodología para derivar una jerarquía de conceptos a partir de una colección de objetos y las relaciones que verifican. De esta forma, el propósito es poder representar y organizar la información de manera más cercana al pensamiento humano sin perder rigor científico. En este sentido se enmarca la cita \begin{acrerror*}
{Falta la referencia}{}
\end{acrerror*}de Rudolf Wille: ``\textit{El objetivo y el significado del FCA como teoría matemática sobre conceptos y sus jerarquías es apoyar la comunicación racional entre seres humanos mediante el desarrollo matemático de estructuras conceptuales apropiadas que se puedan manipular con la lógica.}''

El término FCA fue acuñado por Wille en 1984 culminando años más tarde con la publicación más citada al respecto en colaboración con Bernhard Ganter \cite{Ganter1997}. Desde entonces FCA se ha aplicado con éxito en diferentes disciplinas\crerror*[inline]{}{\textst{ de la Ciencia}}, como por ejemplo: minería de datos, biología celular \cite{Endres2012}, genética \cite{Kaytoue2011}, economía, ingeniería del software \cite{Snelting1998}, medicina \cite{Motameny2008}, derecho \cite{Mimouni2015}, gestión de la información \cite{Priss2006}, etc.

\crerror*[inline]{}{\textst{La motivación principal de }}FCA \crerror*[inline]{parte de una representación de}{\textst{aboga por representar}} conjuntos de objetos y atributos por medio de tablas de datos. Estas tablas se denominan contextos formales y representan las relaciones binarias entre esos objetos y atributos. \crerror*[inline]{A partir de ahí, se generan dos herramientas}{\textst{Principalmente, existen dos formas}} básicas para representar el conocimiento: los retículos de conceptos y los conjuntos de implicaciones. 

\crerror*[inline]{Se pueden encontrar}{\textst{Desde hace años, existen}} en la literatura \begin{acrerror*}
{diferentes}{}
\end{acrerror*}estudios \cite{Kuznetsov2002} donde se han investigado y comparado diferentes algoritmos para obtener el retículo de conceptos a partir del conjunto de datos (en adelante \textit{dataset} por su nomenclatura habitual en el campo). Sin embargo, debido a que el tamaño del retículo de conceptos es, en el peor de los casos, $2^{min(|G|,|M|)}$, siendo $G$ el conjunto de objetos y $M$ el conjunto de atributos, el coste computacional de los métodos encargados de su construcción constituye una limitación de la aplicación de FCA.

Por otro lado \crerror*[inline]{se tiene}{\textst{tenemos}} el conjunto de implicaciones. Las implicaciones pueden considerarse \textit{grosso modo} como reglas del tipo \textit{si-entonces}, o en inglés: \textit{if-then rules}, \crerror*[inline]{representan}{\textst{cuya noción principal es}} un concepto muy intuitivo \begin{acrerror*}
{y sobradamente conocido}{}
\end{acrerror*}: cuando se verifica una premisa, entonces se cumple una conclusión. Esta idea básica se \crerror*[inline]{utiliza con diferentes interpretaciones}{\textst{refleja}} en numerosos campos de conocimiento. Así, en \crerror*[inline]{la teoría relacional de}{\textst{base de datos relacionales}} se \crerror*[inline]{interpretan como}{\textst{denominan}} dependencias funcionales \cite{Codd1970}, en FCA \crerror*[inline]{como}{\textst{son}} implicaciones \cite{Ganter1997} y en programación lógica \crerror*[inline]{como}{\textst{son}} reglas \crerror*[inline]{}{\textst{lógicas de programación}} \cite{Niemela1999}. No obstante, al igual que en el caso del retículo de conceptos, también existen ciertas desventajas a la hora de trabajar con implicaciones, de hecho, la propia extracción del conjunto completo de implicaciones de un dataset es una tarea que presenta una complejidad exponencial \cite{Cohen2001}. 

Trabajar con conjuntos de implicaciones \crerror*[inline]{}{\textst{nos}} permite utilizar técnicas \crerror*[inline]{de razonamiento automático}{\textst{automáticas}} basadas en la lógica\crerror*[inline]{Este hecho fundamenta el objetivo a alto nivel}{\textst{, lo que nos conduce al propósito principal}} de esta tesis doctoral\crerror*[inline]{:}{\textst{, que principalmente consiste en, utilizando los conjuntos de implicaciones,}} aplicar mecanismos lógicos para realizar un tratamiento eficiente de la información \begin{acrerror*}
{partiendo del concepto de implicación.}{}
\end{acrerror*}.

La aproximación a través de la lógica es posible gracias a sistemas axiomáticos válidos y completos como los Axiomas de Armstrong \cite{Armstrong74} y la Lógica de Simplificación \cite{Mora2004} (SL por sus siglas en inglés: \textit{Simplification Logic}). Estos métodos aplicados sobre conjuntos de implicaciones \crerror*[inline]{se aplican}{\textst{nos proporcionan la base para trabajar}} en esta tesis doctoral \crerror*[inline]{en tres áreas de investigación}{\textst{fundamentalmente sobre los siguientes tres campos de conocimiento}}: claves minimales, generadores minimales y sistemas de recomendación conversacionales.

\begin{acrerror*}
{Este párrafo tal como está redactado la verdad es  no aporta mucho en mi opinión. Yo simplemente diría que en los siguiente apartados se describirán los problemas tratados y las soluciones aportadas}{}
\end{acrerror*}En cada uno de los casos, vamos a aprovechar la información subyacente al conjunto de implicaciones, que son el núcleo principal de conocimiento en esta tesis, para realizar novedosas aproximaciones que nos permitan abordar problemas presentes en esos ámbitos. De esta forma, con los métodos desarrollados basados en implicaciones, hemos conseguido obtener resultados favorables para todos esos campos. Tales resultados se sustentan por una amplia gama de experimentos, en los cuales se ha utilizado tanto información real como conjuntos autogenerados y donde la computación paralela en entornos de supercomputación ha desempeñado un papel crucial.

Antes de \crerror*[inline]{describir}{\textst{entrar}} con mayor detalle en los anteriores tres campos, es necesario hacer una importante declaración previa. Tal y como se ha mencionado anteriormente, \crerror*[inline]{en esta tesis se trabaja}{\textst{vamos a trabajar}} con el conjunto de implicaciones que se \crerror*[inline]{derivan de}{\textst{verifican en}} un dataset. Sin embargo, \crerror*[inline]{es conveniente destacar}{\textst{hay que dejar claro}} que no es competencia de este trabajo el \crerror*[inline]{estudio de}{\textst{estudiar}} técnicas \crerror*[inline]{de}{\textst{para la}} extracción de \crerror*[inline]{}{\textst{estas}} implicaciones (lo cual es más una tarea de minería de datos), sino que la intención es partir del \crerror*[inline]{}{\textst{punto en el que ya contamos con el}} conjunto de implicaciones para trabajar con él. A este respecto, podemos mencionar los trabajos más citados en la literatura en relación a la extracción \crerror*[inline]{}{\textst{del conjunto}} de implicaciones a partir de datasets \cite{HuhtalaKPT99,YaoHB2002,Yevtushenko2006}. Son trabajos de suma importancia desde el punto de vista teórico pero también desde el punto de visto práctico, pues incluyen las implementaciones de las aplicaciones que realizan la extracción de las implicaciones. 
%Dicho esto, retomamos el texto pasando a introducir los tres campos de aplicación donde hemos utilizado los conjuntos de implicaciones.

%Sin perjuicio de lo anterior, dado que hemos llegado a crear datasets propios sobre los que poder realizar experimentos como veremos más adelante, si bien no entramos en las técnicas de extracción de implicaciones que utilizan esas aplicaciones, sí hemos tenido que convertirnos en usuarios de estas aplicaciones para poder obtener el conjunto atributos e implicaciones que se verifican. Dicho esto, retomamos el texto pasando a introducir los tres campos de aplicación donde hemos utilizado los conjuntos de implicaciones.

\section{Claves Minimales}
\noindent
El concepto de clave es fundamental en cualquier modelo de datos, incluyendo el modelo de datos relacional de Codd \cite{Codd1970}. Una clave de un esquema relacional está compuesta por un subconjunto de atributos que representan el \textit{dominio} de una determinada función cuya \textit{imagen} es la totalidad del conjunto de atributos. Estas funciones se pueden representar por medio de Dependencias Funcionales (FD, por sus siglas en inglés: \textit{Functional Dependencies}) que especifican una relación entre dos subconjuntos de atributos, e.g. $A$ y $B$, asegurando que para cualesquiera dos tuplas de una tabla de datos, si \begin{acrerror*}
{No soy experto en esta parte, pero el término ``verificar'' en bases de datos no me parece muy intuitivo}{verifican}
\end{acrerror*} $A$, entonces también verifican $B$. Nótese el cambio de denominación de implicación a dependencia funcional por estar en el entorno de los sistemas de base de datos relacionales. 

La identificación de las claves \begin{acrerror*}
{aquí deberías introducir el concepto de ``minimal'' (que por cierto no es un término del español)}{minimales}
\end{acrerror*} de una determinada relación es una tarea crucial para muchas áreas de tratamiento de la información: modelos de datos \cite{Simsion2005}, optimización de consultas \cite{Kemper1991}, indexado \cite{Manolopoulos1999}, etc. El problema consiste en el descubrimiento de todos los subconjuntos de atributos que componen una clave minimal a partir de un conjunto de FD que se cumplen en un esquema de una tabla del modelo relacional.

Las claves no sólo son parte fundamental a considerar durante la fase de diseño de sistemas de base de datos relacionales, sino que también se consideran una poderosa herramienta para resolver multitud de problemas referentes a diversos aspectos del tratamiento de la información. Como muestras de la relevancia de este problema, encontramos numerosas citas en la literatura, entre las que podemos destacar las siguientes. En \cite{Sismanis2006}, los autores afirman que: ``\textit{identification of keys is a crucially important task in many areas of modern data management, including data modeling, query optimization (provide a query optimizer with new access paths that can lead to substantial speedups in query processing), indexing (allow the database administrator to improve the efficiency of data access via physical design techniques such as data partitioning or the creation of indexes and materialized views), anomaly detection, and data integration}''. Más recientemente, en referencia a áreas emergentes como el \textit{linked-data}, en \cite{Pernelle2013}, los autores delimitan el problema manifiestando: \textit{``establishing semantic links between data items can be really useful, since it allows crawlers, browsers and applications to combine information from different sources.''}

Para ilustrar \crerror*[inline]{}{\textst{de forma básica}} el concepto de clave, \crerror*[inline]{sirva}{\textst{vamos a utilizar}} el siguiente Ejemplo \ref{ejemplo:basicoClaves}.

\begin{ejemplo}
\label{ejemplo:basicoClaves}
Supongamos que disponemos de la Tabla \ref{tabla:ejemploPeliculas}. Es una pequeña tabla \crerror*[inline]{con}{\textst{donde se refleja}} información que relaciona títulos de películas, actores, países, directores, nacionalidad y años de estreno. 

De esta información, utilizando los métodos comentados anteriormente, podemos extraer el siguiente conjunto de FDs: 

$\Gamma = \{Titulo, A\tilde{n}o \rightarrow Pais$;  $Titulo, A\tilde{n}o \rightarrow Director$; $Director\rightarrow Nacionalidad$\}. 

\begin{table*}[htbp]
\caption{Tabla de películas}
\label{tabla:ejemploPeliculas}
\centering
{\scriptsize
\begin{tabular}{cccccc}
 \hline
 Título & Año & País & Director & Nacionalidad & Actor\\
 \hline
 Pulp Fiction & 1994 & USA & Quentin Tarantino & USA & John Travolta\\
 Pulp Fiction & 1994 & USA & Quentin Tarantino & USA & Uma Thurman\\
  Pulp Fiction & 1994 & USA & Quentin Tarantino & USA &Samuel Jackson \\
 King Kong & 2005 & NZ & Peter Jackson& NZ & Naomi Watts\\
 King Kong & 2005 & NZ & Peter Jackson & NZ & Jack Black\\
 King Kong & 1976 & USA & De Laurentiis & IT & Jessica Lange\\
 King Kong & 1976 & USA & De Laurentiis & IT & Jeff Bridges\\
 Django Unchained & 2012 & USA & Quentin Tarantino  & USA & Jamie Foxx\\
 Django Unchained & 2012 & USA & Quentin Tarantino  & USA & Samuel Jackson\\\hline
\end{tabular}
}
\end{table*}

Esta tabla tiene una sola clave minimal: $\{Titulo, A\tilde{n}o, Actor\}$ que corresponde con el conjunto de atributos necesario para identificar cualquier tupla de la relación.
\end{ejemplo}

\begin{acrerror*}
{Esto es lo que decía que debe estar antes (desde que se empieza a hablar de claves minimales)}{Una característica muy importante de la claves es su minimalidad. Una clave se considerará minimal cuando todos y cada uno de los atributos que la forman son imprescindibles para mantener su naturaleza de clave, es decir, no contiene ningún atributo superfluo}
\end{acrerror*}. %Formalmente:

%\begin{definicion}[Clave minimal]
%Dada un tabla $R$ sobre un conjunto de atributos $\Omega$, decimos que $K$ es una clave minimal de $R$ si se verifica la dependencia funcional $K\rightarrow \Omega$ en $R$ y $\not\exists a \in K$ tal que $K \smallsetminus \{a\} \rightarrow \Omega$.
%\end{definicion}



% El problema de la búsqueda de claves
\section*{El problema de la búsqueda de claves}
\label{sec:problemaBusquedaClaves}
% Problema de la búsqueda de claves
El problema de la búsqueda de claves consiste en encontrar todos los subconjuntos de atributos que componen una clave mínimal a partir de un conjunto de FD que se verifican en un esquema de una tabla de de datos relacional. Es un campo de estudio con décadas de antigüedad como \crerror*[inline]{puede observarse}{\textst{vemos}} en \cite{Fadous75}, donde las claves se estudiaron dentro del ámbito de la matriz de implicaciones, u otros tantos trabajos como \cite{Sali2004,Giannella99} que se centran en averiguar estas claves minimales.\begin{acrerror*}
{Habría que justificar que es un tema actual  aunque se trabaje desde hace mucho tiempo}{}
\end{acrerror*}

% Claves como elemento crucial
La dificultad \crerror*[inline]{en el}{\textst{al enfrentarnos con el}} problema de la búsqueda de claves surge debido a que, dado un conjunto de atributos $A$, la cardinalidad del conjunto $2^A$ hace que haya que abordar el problema aplicando técnicas que guíen la búsqueda de los conjuntos candidatos a ser claves minimales. \begin{acrerror*}
{Este párrafo no se entiende bien}{}
\end{acrerror*}

% problema NP
El cálculo de \crerror*[inline]{}{\textst{todas}} las claves minimales representa un problema complejo. En \cite{Lucchesi78,Yu76} se incluyen resultados interesantes acerca de la complejidad del problema; los autores demuestran que el número de claves minimales para un sistema relacional puede ser exponencial respecto al número de atributos, o factorial respecto al número de dependencias. Además, establecieron que el número de claves está limitado por el factorial del número de dependencias, por tanto, no existe un algoritmo que resuelva el problema en tiempo polinómico. En definitiva, es un problema NP-completo decidir si existe una clave de tamaño a lo sumo $k$ dado un conjunto de FD \cite{Lucchesi78}.

\begin{acrerror*}
{Esto de la aplicación a liked-data está repetido, ya se ha dicho antes}{Es significativo como el problema de la búsqueda de claves aparece en diversos campos de conocimiento. Por ejemplo, en \cite{Benito-PicazoCMMSE2015} se hace mención a la importancia de conocer las claves en áreas emergentes como el \textit{linked-data}}
\end{acrerror*}. Por otro lado, en \cite{CorderoEMG14}, mostramos cómo el problema de las claves mínimales en las bases de datos tiene su análogo en FCA, donde el papel de las FDs se manifiesta como implicaciones de atributos. En ese artículo, el problema de las claves mínimales se presentó desde un punto de vista lógico y para ello se empleó un sistema axiomático para gestionar las FDs y las implicaciones que los autores denominaron \slfde \cite{Enciso2002}.

% referencias generales
Las principales referencias sobre este problema apuntan \crerror*[inline]{al trabajo}{\textst{a los trabajos}} de Lucchesi y Osborn en \cite{Lucchesi78} que \crerror*[inline]{presenta}{\textst{muestran}} un algoritmo para calcular todas las claves candidatas. Por otro lado, Saiedian y Spencer \cite{Saiedian1996} presentaron un algoritmo usando grafos con atributos para encontrar todas las claves posibles de un esquema de base de datos relacional. No obstante, demostraron que sólo podía aplicarse cuando el grafo de FDs no estuviera fuertemente conectado. \crerror*[inline]{Es reseñable también}{\textst{Otro ejemplo lo encontramos en}} el trabajo de Zhang \cite{Zhang09}, en el cual se utilizan mapas de Karnaugh \cite{Karnaugh1953} para calcular todas las claves. Existen  más trabajos sobre el problema del cálculo de las claves minimales como son \cite{Sismanis2006,Worland2004}, \crerror*[inline]{destacando también una}{\textst{y otra}} contribución actual que aborda el problema en un estilo lógico \cite{CorderoEMG14}. Asimismo, en \cite{Levy2005,Valtchev03,Valtchev08} los autores propusieron el uso de FCA \cite{Ganter1997} para abordar problemas relacionados con la búsqueda y la gestión de las implicaciones, que pueden considerarse complementarios a nuestro trabajo.




\section*{Algoritmos para el cálculo de claves}
\label{sec:algoritmosCalculoClaves}
% referencias de tableaux
\crerror*[inline]{En este ámbito, el objetivo de esta tesis se centra}{\textst{Por nuestra parte, como objetivo, nos hemos centrado}} en los algoritmos de búsqueda de claves basados en la lógica, y más específicamente, en aquellos que utilizan el paradigma de Tableaux \cite{Morgan1992,Risch1992} \crerror*[inline]{como}{\textst{utilizando un}} sistema de inferencia. 

De forma muy general, \crerror*[inline]{se puede}{\textst{podemos}} decir que los métodos tipo Tableaux representan el espacio de búsqueda como un árbol, donde sus hojas contienen las soluciones (claves). El proceso de construcción del árbol comienza con una raíz inicial y desde allí, las reglas de inferencia generan nuevas ramas etiquetadas con nodos que representan instancias más simples del nodo padre. \begin{acrerror*}
{No queda claro en estas líneas por qué es más versátil y por qué las comparaciones son fáciles}{La mayor ventaja de este proceso es su versatilidad, ya que el desarrollo de nuevos sistemas de inferencia nos permiten diseñar un nuevo método. Las comparaciones entre estos métodos se pueden realizar fácilmente ya que su eficiencia va de la mano con el tamaño del árbol generado.}
\end{acrerror*}

Esto \crerror*[inline]{En adelante no marco más cosas del uso de la primera persona. Revisar en general}{\textst{nos}} lleva a un punto de partida fundamental, los estudios de R. Wastl (Universidad de Wurzburg, Alemania) \cite{Wastl98a,Wastl98} donde se introduce por primera vez un sistema de inferencia de tipo Hilbert para averiguar todas las claves de un esquema relacional. \begin{acrerror*}
{En mi opinión, o explicaría brevemente cómo interpretar el gráfico o lo quitaría}{A modo de ejemplo básico, en la Figura \ref{figura:ejemploTaleaux} podemos ver un ejemplo de árbol de búsqueda según el paradigma de Tableaux desarrollado según las reglas de inferencia del sistema de inferencia $\mathbb{K}$ de Wastl.}
\end{acrerror*}

\begin{figure}[htbp]
	\begin{center}
		\includegraphics*[width=.75\textwidth,height=.3\textheight]{arbol897.png}
	\end{center}
	\caption{Ejemplo de Tableaux utilizando el sistema de inferencia $\mathbb{K}$ de Wastl.}
	\label{figura:ejemploTaleaux}
\end{figure}

Siguiendo esta línea, en \cite{Cordero2013} los autores abordan el problema de la búsqueda de claves utilizando un sistema de inferencia basado en la lógica de simplificación para dependencias funcionales \cite{Enciso2002} demostrando como el árbol del espacio de búsqueda que se genera nos lleva a sobrepasar las capacidades de la máquina, incluso para problemas pequeños. En \cite{Mora2012} los autores muestran la equivalencia entre \slfde y los axiomas de Armstrong \cite{Armstrong74} junto con un algoritmo para calcular el cierre de un conjunto de atributos. Más tarde, en \cite{CorderoEMG14}, los autores introdujeron el método SST, basado en la introducción del test de minimalidad que evita la apertura de ramas adicionales del árbol, por lo que el espacio de búsqueda se vuelve más reducido, logrando un gran rendimiento en comparación con sus predecesores.

% con el paralelo
Una propiedad muy interesante de los métodos basados en Tableaux es \crerror*[inline]{la}{\textst{su}} generación de subproblemas independientes los unos de los otros a partir del problema original. De esta forma llegamos a nuestro objetivo fundamental de utilizar las técnicas lógicas sobre una implementación paralela de los métodos que, mediante el uso de recursos de supercomputación, nos permitan alcanzar resultados en un tiempo razonable.

En esta línea, son varios los trabajos que han utilizado la paralelización para afrontar problemas relacionados con implicaciones o FCA. Un algoritmo paralelo para el tratamiento de implicaciones enmarcado en el campo de los hipergrafos lo podemos encontrar en \cite{Sridhar1990}. A su vez, Krajca et al. \cite{Krajca2008} presentan un algoritmo paralelo para el cálculo de conceptos formales. Por nuestra parte, \begin{acrerror*}
{Mejorar redacción}{una primera aproximación a la paralelización del método de Wastl \cite{Wastl98a,Wastl98} y el algoritmo de claves \cite{Cordero2013}}
\end{acrerror*} fue presentado en \cite{Benito-Picazo2014}, donde \begin{acrerror*}
{Describir la aportación de forma un poco más ``formal''}{se muestra cómo el paralelismo puede integrarse de forma natural}
\end{acrerror*} en los métodos basados en Tableaux.

Para la labor de computación de alto rendimiento, \crerror*[inline]{se ha colaborado}{\textst{a lo largo de este trabajo se han adquirido y aplicado las competencias necesarias para poder trabajar en entornos de supercomputación; en concreto, durante los últimos años, se ha trabajado fervientemente}} con el Centro de Supercomputación y Bioinnovación de la Universidad de Málaga\footnote{http://www.scbi.uma.es/}. La posibilidad de tratar con este Centro \crerror*[inline]{ha permitido}{\textst{nos ha proporcionado dos beneficios fundamentales: por un lado, se ha alcanzado una elevada pericia para trabajar en entornos de HPC y para realizar implementaciones que aprovechen una alta cantidad de recursos, y por otro lado, se han podido}} obtener resultados empíricos sobre experimentos utilizando estrategias de paralelismo, que han \crerror*[inline]{avalado}{\textst{desembocado en}} contribuciones científicas \cite{Benito-Picazo2014,Benito-Picazo2016} \crerror*[inline]{}{\textst{y}} que habría sido imposible conseguir en la actualidad sin contar con tales recursos computacionales.


\section*{Métodos \textit{SST} y \textit{CK}}
\label{sec:metodosSSTyCK}
En \cite{CorderoEMG14} se presentó un nuevo algoritmo, denominado SST, para calcular todas las claves minimales usando una estrategia de estilo Tableaux, abriendo la puerta a incorporar el paralelismo en su implementación. SST se basa en la noción de cierre de conjunto, una noción básica en la teoría de \crerror*[inline]{bases}{\textst{base}} de datos que permite caracterizar el \begin{acrerror*}
{Mejorar redacción}{conjunto máximo de atributos que se puede alcanzar, desde un determinado conjunto de atributos $A$ con respecto a un conjunto de FD, utilizando el sistema axiomático.}
\end{acrerror*} Por lo tanto, si el cierre de $A$ se denota como $A^+_\Gamma$, el sistema de inferencia para FD nos permite inferir la FD $A\to A^+_\Gamma$. El enfoque con estilo lógico para el problema de las claves minimales consiste en la enumeración de todos los conjuntos de atributos $A$ tales que se verifique la FD: $A\to\Omega$.

SST muestra un gran rendimiento en comparación con sus predecesores \crerror*[inline]{,}{\textst{como hemos visto hasta ahora y}} como puede comprobarse en el amplio estudio realizado sobre el método en \cite{Benito-Picazo2014TFM}. El beneficio principal en la reducción del espacio de búsqueda \crerror*[inline]{se}{\textst{de}} debe a la introducción \crerror*[inline]{de un}{\textst{del}} test de inclusión para evitar la apertura de ramas extra. Gracias a ello, SST no abre algunas ramas que sabemos que van a producir las mismas claves que se calculan en otra rama.
\begin{acrerror*}
{Creo que aquí habrá que destacar que se pasa de hablar del estado del arte y se empieza a hablar de la aportación, y encuadrarla bien}{}
\end{acrerror*}

Basándonos en el sistema axiómatico de la lógica \slfde \crerror*[inline]{(véase el apartado }{\textst{}}\ref{sec:logicaSimplificacion}, proponemos un nuevo método llamado \textit{Closure Keys (CK)} que incorpora un mecanismo eficiente de poda que utiliza el método de cierre basado en \slfde para mejorar el método SST. El nuevo operador de cierre definido en \cite{Mora2012} permite reducir el espacio de búsqueda realizando reducciones en el camino hacia las hojas, donde finalmente se obtienen las claves.  El método CK tiene una característica fundamental que lo convierte en un novedoso enfoque \crerror*[inline]{frente a}{\textst{a}} los métodos clásicos de cierre, ya que proporciona una nueva funcionalidad: \begin{acrerror*}
{Esto queda muy poco claro, mejorar redacción}{además del conjunto de atributos que constituye la salida del operador de cierre, el método proporciona un subconjunto de implicaciones del conjunto $\Gamma$ original.}
\end{acrerror*}

Por tanto, el método CK recibe un conjunto de implicaciones $\Gamma$ y un subconjunto de atributos $X \subseteq \Omega$, calcula el conjunto cierre $X^+$ respecto a $\Gamma$, y además, un nuevo conjunto $\Gamma^\prime$ que contiene el conjunto de implicaciones que guarda la semántica que queda fuera del cierre $X^+$. Si $\Gamma^\prime = \varnothing$, entonces $X^+ = \Omega$ (véase \cite{Mora2012} para más detalles).

\crerror*[inline]{Con esto, se tienen los elementos para presentar}{\textst{Finalmente}}, la principal contribución \crerror*[inline]{de la tesis en la resolución del problema de la búsqueda de claves
}{\textst{de esta parte de la tesis se produce de la siguiente forma}}. \begin{acrerror*}
 {Yo no presentaría la aportación como una implementación (puede parecer poca cosa), sino como el diseño de un algoritmo}{Se ha desarrollado una implementación}
 \end{acrerror*} paralela tanto del método SST como del método CK basándose en el paradigma \textit{MapReduce} \cite{Dean2004}. Básicamente, \begin{acrerror*}
 {Entiendo que se refiere al algoritmo tuyo}{el algoritmo paralelo de búsqueda de claves}
 \end{acrerror*} se divide en dos partes principales. \crerror*[inline]{}{\textst{Utiliza}} una primera fase en la que se realiza una expansión del árbol de búsqueda trabajando sobre el problema original pero llegando únicamente hasta un cierto nivel de árbol, es decir sin alcanzar todavía las claves en las hojas del árbol. En ese momento interviene la segunda etapa en la que cada nodo de ese nivel del árbol, que constituye un problema equivalente al original pero simplificado como resultado de la aplicación de las reglas de inferencia hasta ese momento, se resuelve en paralelo mediante el uso de un elevado número de cores, es decir, aplica el algoritmo de búsqueda de claves, pero ahora sí, hasta alcanzar las hojas del árbol.
 \begin{acrerror*}
 {Si es una de las aportaciones principales habría que describirlo mejor...¿de dónde viene la mejora?,  ¿Por qué se llega sólo hasta un cierto nivel en la primera fase?, ¿cómo se decide ese nivel?,... Al ser una tesis por compendio creo que aún es más importante que en la introducción quede muy claro lo que se ha hecho}{}
 \end{acrerror*}

\crerror*[inline]{Para contrastar la aportación del algoritmo,}{\textst{De esta forma}} se han realizado multitud de \crerror*[inline]{pruebas, las}{\textst{experimentos para confirmar las mejoras que se han alcanzado, los}} cuales \crerror*[inline]{requieren ejecutarse}{\textst{necesitan llevarse a cabo}} en entornos de supercomputación y cuyos resultados pueden consultarse en \cite{Benito-Picazo2016}. \crerror*[inline]{Así}{\textst{Principalmente}}, se ha demostrado \crerror*[inline]{que el algoritmo diseñado es claramente adecuado para}{\textst{como estos métodos son claramente adecuados para ejecutarse utilizando}} una implementación paralela. De esta forma, se consiguen resultados en tiempos razonables incluso \crerror*[inline]{en casos en los que la}{\textst{cuando la}} cantidad de información de entrada es considerable y \crerror*[inline]{en los que}{\textst{donde}} los métodos secuenciales no son capaces de finalizar.


\section{Generadores Minimales}
Como se ha mencionado anteriormente, una forma de representar en FCA el conocimiento es el retículo de conceptos. Esta representación otorga una visión global de la información con un formalismo muy \crerror*[inline]{sólido}{\textst{fuerte}}, abriendo \crerror*[inline]{la}{\textst{el}} puerta para utilizar la teoría de retículos como una metateoría para gestionar la información \cite{Bertet2016}.

Los conjuntos cerrados son la base para la generación del retículo de conceptos ya que éste puede ser construido a partir de \crerror*[inline]{aquellos}{\textst{los conjuntos cerrados}}, considerando la relación de subconjuntos como \crerror*[inline]{}{\textst{la}} relación de orden. En este punto nace el concepto de generadores minimales como aquellas representaciones de los conjuntos cerrados que no contengan información \crerror*[inline]{superflua}{\textst{supérflua}}, es decir, representaciones canónicas de cada conjunto cerrado \cite{Ganter1997}.

Los generadores minimales junto con los conjuntos cerrados son esenciales para obtener una representación completa del conocimiento en FCA, pero no sólo son interesantes desde un punto de visto teórico. La importancia de los generadores minimales puede apreciarse claramente a través de citas tales como: \textit{``Minimal generators have some nice properties: they have relatively small size and they form an \begin{acrerror*}
{¿Está bien esto de ``order ideal''}{order ideal}
\end{acrerror*}''}, existente en importantes estudios como el de Poelmans et al. \cite{Poelmans2013} o \cite{Qu2007}. Además, los generadores minimales se han usado como punto clave para generar bases, las cuales constituyen una representación compacta del conocimiento que facilita un mejor rendimiento de los métodos de razonamiento basados en reglas. Missaoui et al. \cite{Missaoui2010,Missaoui2012} presentan el uso de generadores minimales para calcular bases que impliquen atributos positivos y negativos cuyas premisas son generadores minimales.

Cualquier operador de cierre $c$ en \begin{acrerror*}
{¿Qué es M? Ojo al usar la notación, es necesario describirla}{$M$}
\end{acrerror*} puede asociarse con un sistema de implicaciones. Esta conexión establece una forma de gestionar el trabajo del operador de cierre $c$ por medio de su derivación sintáctica y, como consecuencia, se puede elaborar un método para realizar esta gestión. Pero, ¿qué pasa con la conexión inversa? Es decir, dado un conjunto de implicaciones, ¿es posible generar el operador de cierre $c$ asociado a él? Tal pregunta es el núcleo de \begin{acrerror*}
{No me gusta lo de esta ``parte'' de la tesis}{esta parte de la tesis}
\end{acrerror*} y su solución implica enumerar todos los conjuntos cerrados.
\begin{acrerror*}
{Me ocurre igual que antes: me gustaría una diferenciación más clara entre el estado del arte y las aportaciones de la tesis, que deberían quedar expuestas más claramente}{}
\end{acrerror*}

Si tenemos que $X,Y \subseteq M$ satisfacen que $X = Y^+_\Sigma$, es habitual decir que $Y$ es un generador del conjunto cerrado $X$. Obsérvese que cualquier subconjunto de $X$ que contiene $Y$ es también un generador de $X$. Dado que trabajamos con conjuntos finitos de atributos, el conjunto de los generadores de un conjunto cerrado se pueden caracterizar por sus generadores minimales.


\section*{Métodos para el cálculo de generadores minimales}
\label{seccion:metodosGeneradoresMinimales}
En nuestro caso, centrándonos una vez más en el tratamiento inteligente de los conjuntos de implicaciones, vamos a utilizarlos como los elementos para describir la información y además, como base del diseño de métodos para enumerar todos los conjuntos cerrados y sus generadores minimales a partir de esta información, \begin{acrerror*}
{Algún punto para cortar el párrafo, please!!}{}
\end{acrerror*}y no del \textit{dataset} original (como en los \begin{acrerror*}
{¿Cuáles trabajos, dónde se han mencionado?}{trabajos mencionados}
\end{acrerror*}), lo cual, hasta donde sabemos, no \crerror*[inline]{se había hecho previamente}{\textst{existe trabajo previo al respecto}}. 

El \crerror*[inline]{}{\textst{nuevo}} método propuesto \crerror*[inline]{en esta tesis}{\textst{}}es una evolución del \crerror*[inline]{}{\textst{método}} presentado en \cite{Cordero2012} \crerror*[inline]{,}{\textst{}} donde se utilizó \crerror*[inline]{}{\textst{la}} \slfde como herramienta para encontrar todos los generadores minimales a partir de un conjunto de implicaciones. Este método trabaja sobre el conjunto de implicaciones aplicando unas reglas de inferencia y construyendo un \begin{acrerror*}
{¿espacio de búsqueda en árboles? ¿un árbol?}{espacio de búsqueda de árbol}
\end{acrerror*} muy parecido a los árboles del caso de las claves minimales.

Concretamente, dado un conjunto de atributos $M$ y un sistema de implicaciones $\Sigma$, el método realiza un mapeo $mg_\Sigma\colon 2^M\to 2^{2^M}$ que satisface la siguiente condición.

$$\forall X,Y\subseteq M$$

\begin{center}
$X\in mg_\Sigma(C)$ si y sólo si $C$ es cerrado para $(\ )^+_\Sigma$ y $X$ es un generador minimal para $C$.
\end{center}

\begin{ejemplo}
Sea $\Sigma=\{a\to c, bc\to d, c\to ae, d\to e\}$, el mapeo $mg_\Sigma$ se describe como:
\begin{center}
\begin{tabular}{p{1.6cm}|p{.4cm}p{.3cm}p{.3cm}p{.5cm}p{.5cm}p{.6cm}p{.6cm}p{.8cm}p{.9cm}}
$X$ & $\varnothing$ & $b$ & $e$ & $be$ & $de$ & $ace$ & $bde$ &  $acde$ &   $abcde$   \\
\hline
$mg_\Sigma(X)$ &$\varnothing$ & $b$ & $e$ & $be$ & $d$  & $a$ & $bd$ & $ad$ & $ab$ 
\\
& & & & & & $c$& &$cd$ & $bc$    
\end{tabular}
\end{center}

En otro caso, $X$ no es cerrado y $mg_\Sigma(X)=\varnothing$. Nótese que $\varnothing$ es cerrado y $mg_{\Sigma}(\varnothing)=\{\varnothing\}$, i.e. $\varnothing$ es un generador minimal del conjunto cerrado $\varnothing$.
\end{ejemplo}

Tras el método presentado en \cite{Cordero2012} (que denominaron MinGen) se presenta ahora un nuevo método (MinGenPr) que aplica una importante mejora con respecto al anterior. Básicamente consiste en incorporar un mecanismo de poda, basada en el test de inclusión de conjuntos que involucra a todos los nodos del mismo nivel, para evitar la generación de generadores minimales y cierres redundantes. El propósito de esta poda es verificar la información de cada nodo en el espacio de búsqueda, evitando la apertura de una rama completa. Finalmente, se propone un último método (GenMinGen) que generaliza la estrategia de poda anterior al considerar el test de inclusión del subconjunto no sólo con la información de los nodos del mismo nivel, sino también con todos los generadores minimales calculados antes de la apertura de cada rama.

En definitiva, se han \begin{acrerror*}
{Como dije antes, me gusta más decir que se ha diseñado el método, la implementación en sí no es ``investigación''}{estudiado e implementado}
\end{acrerror*} cada uno de estos métodos en su versión secuencial, y para evaluar el rendimiento e ilustrar las mejoras obtenidas al pasar de un método a otro, se han realizado un gran número de pruebas utilizando información \begin{acrerror*}
{Aclarar esto, ¿autogenerada quiere decir aleatoria? ¿la real de qué contexto viene?}{autogenerada e información real}
\end{acrerror*}.



\section*{Generadores minimales y paralelismo}
\label{seccion:generadoresMinimalesParalelismo}
La \begin{acrerror*}
{¿La contrapartida de qué?}{contrapartida}
\end{acrerror*} es que la obtención de todos los conjuntos cerrados y sus respectivos generadores minimales es un problema con complejidad exponencial. Sin embargo, dado que nuestro objetivo es explotar las posibilidades de operar con conjuntos de implicaciones, en este trabajo vamos a \begin{acrerror*}
{¿Qué aspectos se van a combinar?}{combinar ambos aspectos}
\end{acrerror*} enumerando todos los conjuntos cerrados a partir de un conjunto dado de implicaciones. De hecho\crerror*[inline]{}{\textst{,}} iremos un paso más allá, calculando no sólo todos los conjuntos cerrados, sino que para cada uno de ellos \crerror*[inline]{calcularemos}{\textst{produciremos}} sus respectivos generadores minimales.

No obstante, vamos a encontrarnos con un problema similar al que sucedía con las claves minimales, y es que, al tratar con grandes cantidades de información, los métodos realizados para producir los generadores minimales, conllevan unas necesidades de cómputo que sobrepasan \begin{acrerror*}
{¿de qué máquina? En una tesis hay que expresarse en términos más rigurosos}{los límites de la máquina}
\end{acrerror*}. Por tanto, una vez más, tenemos que trasladar las implementaciones de los métodos a versiones paralelas que nos permitan funcionar bajo arquitecturas de supercomputación. 

En este sentido, se han \begin{acrerror*}
{Lo mismo de antes de las implementaciones. Hay que dejar claro qué métodos son aportaciones de la tesis (no sólo las implementaciones)}{realizado tanto las implementaciones}
\end{acrerror*} secuenciales como paralelas de los métodos de producción de generadores minimales (MinGen, MinGenPr, GenMinGen, MinGenPar). Para las versiones paralelas vamos a utilizar la misma filosofía de implementación que en el caso de las claves minimales, es decir, hemos desarrollado unos códigos, basándonos en el esquema \textit{MapReduce} \cite{Dean2004}, que se ejecutarán en dos etapas. Al igual que en las claves minimales, la primera etapa divide el problema de entrada en varios subproblemas equivalentes pero reducidos de forma que puedan ser tratados de forma independiente. En la segunda etapa, cada uno de estos subproblemas se resuelve en paralelo usando múltiples núcleos.

Una vez más, para verificar el rendimiento y la idoneidad de los métodos para aplicar estrategias paralelas, se ha realizado una amplia batería de pruebas tanto sobre \begin{acrerror*}
{Lo mismo de antes}{información autogenerada como información real}
\end{acrerror*}. Además, las pruebas han incluido tareas de estimación del número óptimo de \begin{acrerror*}
{Uniformar nomenclatura: o cores o núcleos}{cores}
\end{acrerror*} a utilizar así como del valor de corte más apropiado en la etapa primera de los métodos paralelos. Los resultados obtenidos\crerror*[inline]{}{\textst{ al respecto}} pueden consultarse en \cite{Benito-PicazoCMMSE2017}. 




\section{Sistemas de Recomendación Conversacionales}
\noindent
La \crerror*[inline]{tercera aportación de}{\textst{última aplicación que se ha llevado a cabo en}} esta tesis doctoral \crerror*[inline]{se enmarca}{\textst{haciendo uso de los conjuntos de implicaciones y los conjuntos cerrados se produce}} en el campo de los sistemas de recomendación (SR)\crerror*[inline]{, y también se basa en el uso de los conjuntos de implicaciones y los conjuntos cerrados.}{\textst{.}} 

%El objetivo principal de los SR es ayudar al usuario a elegir entre un número alto de alternativas.  

De forma muy \crerror*[inline]{resumida}{\textst{básica}}, podríamos considerar que un SR es un sistema inteligente que proporciona a los usuarios una serie de sugerencias personalizadas (recomendaciones) \crerror*[inline]{seleccionadas de un conjunto}{\textst{sobre un determinado tipo}} de elementos (ítems). \crerror*[inline]{Comúnmente}{\textst{De forma general}}, \begin{acrerror*}
{En realidad esta frase dice prácticamente los mismo que la anterior}{los SR estudian las características de cada usuario e ítem}
\end{acrerror*} \crerror*[inline]{}{\textst{del sistema}}, y mediante un procesamiento de los datos, encuentra un subconjunto de ítems que pueden resultar de interés para el usuario. Una de las referencias más notables en el campo de los SR la encontramos en el libro de Adomavicius y otros \begin{acrerror*}
{Esta referencia está incompleta}{\cite{AdomaviciusBook11}.}
\end{acrerror*}

% Historia
Desde \crerror*[inline]{los primeros trabajos sobre SRs}{\textst{que el primer SR hizo su aparición en el mundo de las tecnologías de la información}} \cite{Hill1995,Resnick1997}, \crerror*[inline]{éstos}{\textst{los SR}} han estado en continua evolución \crerror*[inline]{}{\textst{durante los últimos años}} \begin{acrerror*}
{No queda bien poner una cita de 2005 si quires referirte a``los últimos años´´}{}
\end{acrerror*}\cite{Adomavicius2005}
. Sin embargo, es con la expansión de las nuevas tecnologías cuando han tenido un acercamiento más directo a la mayor parte de la sociedad debido a su capacidad para realizar todo tipo de recomendaciones sobre \crerror*[inline]{productos muy populares}{\textst{diversos elementos al alcance de todos}} (libros \cite{Crespo2011}, documentos \cite{Porcel2012}, música \cite{LampropoulosLT12}, turismo \cite{BorrasFPMVIORC11}, películas\footnote{https://www.movielens.org}, etc.).

% Importancia
\begin{acrerror*}
{Este párrafo vuelve a decir más o menos lo mismo que el anterior, sería mejor combinarlos}{}
\end{acrerror*}
En la actualidad, los SR constituyen un claro campo de investigación y estudio como demuestran el gran número de trabajos que se están realizando \cite{Son2018,Eirinaki2018} y cuya cantidad continúa aumentando día a día. Además, la relevancia de estos sistemas no se limita al ámbito investigador. Actualmente, muchos SR ya han sido implantados con éxito en fuertes entornos comerciales a nivel mundial. Este es el caso de empresas líderes en el sector como pueden ser Amazon \cite{linden2003}, LinkedIn \cite{Metaphor2012} o Facebook \cite{Tiroshi11}, que han realizado fuertes inversiones con el fin de generar mejores SR. Estas situaciones ponen de manifiesto la gran importancia de estos sistemas en ambas vertientes de la sociedad actual.

% FCA y SR
\begin{acrerror*}
{Creo que si estamos contando la ``historia'' de los SR, sería mejor contar primero los tipos estándar de SR y luego los basados en FCA (justo después de comentar los knowledge-based. Y esto lo diría aquí literalmente con una frase para que el lector sepa el "argumento" que seguimos}{}
\end{acrerror*}
Abordar la generación de recomendaciones haciendo uso de FCA es una aproximación existente en la literatura desde hace años. En \cite{duBoucherRyan2006}, los autores utilizan FCA para agrupar elementos y usuarios en conceptos para posteriormente, realizar recomendaciones colaborativas según la afinidad con los elementos vecinos. Más tarde, en \begin{acrerror*}
{A esta referencia le falta el año. Revisa las referencias en general}{\cite{Senatore2013}}
\end{acrerror*}, los autores introducen un modelo para el filtrado colaborativo basado en FCA para generar correlaciones entre datos a través de un diseño del retículo. Zhang et al. \cite{Zhang2015} propusieron un sistema basado en similitud agrupando la información contextual en grafos mediante el cual llevar a cabo recomendaciones sobre las interacciones sociales entre usuarios. En \cite{LeivaERCMG13,LeivaERCMG13a}, se utilizan relaciones difusas e implicaciones ponderadas para especificar el contexto y \slfde para desarrollar un proceso lineal de filtrado que permite a los SR podar el conjunto original de elementos y así mejorar su eficiencia. Recientemente, en \cite{Zou2017} se propone y utiliza un novedoso SR personalizado basado en el retículo de conceptos para descubrir información valiosa de acuerdo con los requisitos e intereses de los usuarios de forma rápida y eficiente. Todos estos trabajos subrayan claramente cómo FCA puede aplicarse con éxito en el campo de los SR.


\section*{Técnicas de recomendación}
\label{seccion:tecnicasRecomendacion}
% Tipos
\begin{acrerror*}
{Veo que introduces algunas siglas como CF o CB, que me da la impresión que luego no se usan. Si es así elimínalas}{}
\end{acrerror*}

\begin{acrerror*}
{Si estos tipos son los más importantes debes incluir citas relevantes}{}
\end{acrerror*}
Existen numerosos tipos \crerror*[inline]{}{\textst{diferentes}} de SR que normalmente se clasifican atendiendo a cómo se \crerror*[inline]{generan}{\textst{llevan a cabo}} las recomendaciones. Los más conocidos y extendidos son los sistemas de filtrado colaborativo (denominados CF, por sus siglas en inglés: \textit{Collaborative Filtering})\crerror*[inline]{, que }{\textst{ }}basan su funcionamiento fundamentalmente en las valoraciones que otros usuarios han otorgado a los elementos disponibles; y los sistemas basados en contenido (denominados CB, por sus siglas en inglés: \textit{Content-Based}) que \crerror*[inline]{proporcionan }{\textst{se basan en categorizar los ítems a recomendar, proporcionando }}resultados que tengan características similares a otros que han sido bien valorados anteriormente por el usuario.

En los últimos años ha habido un gran crecimiento de los SR contextuales \cite{BenSassi2017}, capaces de tener en cuenta información relevante para la recomendación como puede ser la hora, el lugar, la compañía, la ubicación, etc. \crerror*[inline]{Existen también los conocidos como }{\textst{Los}} SR demográficos \cite{BeelLNG13} que clasifican a los usuarios según diferentes parámetros personales (edad, localización, etc.)\crerror*[inline]{y de acuerdo con esto generan las recomendaciones}{\textst{ }}. Por otro lado encontramos los denominados SR basados en conocimiento (KB, por sus siglas en inglés: \textit{Knowledge-Based}) \cite{Mandl2011}. Estos sistemas\crerror*[inline]{ modelan y }{\textst{ }}gestionan el conocimiento inherente a los datos y revelan cómo un ítem puede satisfacer la necesidad del usuario, es decir, utilizan un método de razonamiento para inferir la relación entre una necesidad y una posible recomendación. 

\begin{acrerror*}
{Creo que sería bueno comentar que los tipos anteriores están determinado por el los datos de partida y el algoritmo que se utiliza, mientras que los conversacionales se diferencian por el procedimiento o flujo de trabajo que se sigue para generar la recomendación}{Finalmente}
\end{acrerror*}, llegamos a los SR más importantes desde el punto de vista de este trabajo, los denominados SR conversacionales \cite{Griol2018,Lee2017}. Estos SR están estrechamente relacionados con los conceptos de \begin{acrerror*}
{En estos dos pondría también los términos en inglés, para que el lector los identifique}{recomendador basado en críticas \cite{Chen2012} y recomendaciones de información}
\end{acrerror*} \cite{TrabelsiWBR11}. Resaltamos este tipo de SR porque será \crerror*[inline]{en el que se enmarca}{\textst{la estrategia principal sobre el que se sustenta}} el desarrollo de SR realizado y que ha dado lugar a una de las contribuciones que avalan esta tesis \cite{Benito-Picazo2017}. 

\begin{acrerror*}
{Estas referencias (y el parrafillo ``reconvertido'' lo podría al comiento de esta sección, para dejar claro que esta clasificación es la estándar y que no nos la hemos inventado nosotros)}{Se puede consultar una clasificación más detallada en el libro de Adomavicius y Tuzhilin \cite{AdomaviciusBook11} que, junto con la contribución de Bobadilla et al. \cite{Bobadilla2013}, constituyen las referencias más citadas en el campo.}
\end{acrerror*}

Podemos apreciar que existen diferentes tipos de estrategias para los SR, sin embargo, la historia ha demostrado ampliamente que la mejor alternativa consiste en combinar características de diferentes tipos de SR para generar híbridos que se beneficien de las ventajas de cada uno de ellos \cite{DeCampos2010}. Tal es nuestro caso, en el que nuestro trabajo ha culminado en un SR híbrido que combina las siguientes técnicas de recomendación:

\begin{itemize}
	\item \textbf{SR basados en conocimiento.} \crerror*[inline]{Ya que se utilizan mecanismos}{\textst{En virtud de nuestro estudio}} de extracción de conocimiento de los datos utilizando FCA, los conjuntos de implicaciones y los operadores de cierre.
	\item \textbf{SR basados en contenido.} \crerror*[inline]{Puesto que}{\textst{Necesario ya que para aplicar}} las técnicas lógicas empleadas \crerror*[inline]{se basan en información sobre los ítems a recomendar y sus atributos}{\textst{necesitamos en primera instancia información sobre la que trabajar}}. 
	\item \textbf{SR conversacionales.} \crerror*[inline]{Ya que es precisamente un proceso de diálogo el que se utiliza apra generar las recomendaciones}{\textst{Se presenta como estrategia central sobre la que aplicar y utilizar las dos anteriores respectivamente}}.
\end{itemize}

Pero no nos ocuparemos únicamente de la estrategia de recomendación sino también del proceso de cómo obtener una recomendación. En este sentido, se introduce el concepto de Recuperación de Información (IR por sus siglas en inglés, \textit{Information Retrieval}), \begin{acrerror*}
{Esto queda muy ambiguo, mejorar la redacción para qeu quede más precisa}{que básicamente se encarga de realizar búsquedas para determinar cuán bien responde cada objeto a una consulta}
\end{acrerror*}. Numerosos trabajos en la literatura relacionan el uso de FCA en modelos basados en IR \cite{Codocedo2015,Ignatov2015}.


\section*{Problemas comunes}
\label{seccion:problemasRecomendacion}
\begin{acrerror*}
{Buscar un título más preciso. Aunque al principio enumeres varios problemas, lo que se hace en esta sección es describir el problema que se intenta resolver en la tesis en cuanto a recomendación}{}
\end{acrerror*}

Si bien es cierto que los SR están alcanzando una enorme importancia\crerror*[inline]{, }{\textst{}} existen numerosas dificultades que han de afrontarse a la hora de \crerror*[inline]{diseñarlos e implementarlos}{\textst{diseñar e implementar un SR}}. \crerror*[inline]{Entre ellas podemos destacar las siguientes}{\textst{En la lista de problemas relacionados con los SR}} \cite{Shah2016} podemos encontrar: el arranque en frío \cite{Feil2016,Son201687}, privacidad \cite{Friedman2015}, oveja-negra \cite{Gras2016}, \begin{acrerror*}
{Creo que sparsity puede traducirse mejor como dispersión de datos}{escasez}
\end{acrerror*} \cite{Guo2012}, ataques maliciosos \cite{Zhou2015,Yang2016}, sobreespecialización \cite{LopsGS11}, escalabilidad \cite{Isinkaye2015}, postergación \cite{Sundaresan2011}, dimensionalidad \cite{Salimi2017}, etc.\begin{acrerror*}
{En alguno de estos problemas vendría bien un par de líneas describiéndolos}{}
\end{acrerror*}

En concreto, nuestro trabajo ha estado orientado a abordar este último problema de la dimensionalidad en los SR. Este problema, también conocido como \textit{the curse of dimensionality phenomenon} \cite{Salimi2017,Nagler2016} aparece cuando es necesario trabajar sobre datasets con un alto número de características (variables o atributos). De forma intuitiva, podríamos \crerror*[inline]{describirlo}{\textst{introducirlo}} de la siguiente manera\crerror*[inline]{: c}{\textst{. C}}uando hay pocas columnas de datos, \crerror*[inline]{}{\textst{es relativamente fácil para}} los algoritmos \crerror*[inline]{}{\textst{realizar tareas}} de tratamiento inteligente de la información \crerror*[inline]{()}{\textst{como: }}aprendizaje automático, \textit{clustering}, clasificación, etc.\crerror*[inline]{) suelen tenre un buen comportamiento. }{\textst{ }}Sin embargo, a medida que aumentan las columnas o características de nuestros ítems, \begin{acrerror*}
{Redactar esto de forma más precisa, cuando hablas de exponencial, ¿a qué te refieres? ¿a la complejidad? Ten en cuenta que la complejidad de un algoritmo no depende del conjunto de datos al que se aplique...}{se vuelve exponencialmente más difícil hacer labores predictivas con un buen nivel de precisión. El número de filas de datos necesarias para realizar cualquier modelado útil aumenta exponencialmente a medida que agregamos más columnas a una tabla.}
\end{acrerror*}

Para abordar este problema, podemos encontrar \begin{acrerror*}
{Incluir referencias}{muchos trabajos en la literatura}
\end{acrerror*} sobre la reducción de la dimensión de la información, especialmente mediante\crerror*[inline]{ técnicas de }{\textst{ }}selección de características \textit{(feature selection)}, que pueden ayudarnos a descartar aquellas características que no son \crerror*[inline]{relevantes de cara al objetivo buscado}{\textst{merecedoras de ser considerados según diferentes criterios}}. De hecho, estas técnicas ya se aplican en otras áreas como \crerror*[inline]{}{\textst{son:}} algoritmos genéticos o redes neuronales, normalmente centrándose en la aplicación de un proceso automatizado que se aplique de una vez \textit{(batch mode)} \crerror*[inline]{}{\textst{mediante selección de características}}.

%Suele ser habitual que para realizar una selección de características por parte del SR, el usuario tenga que introducir y seleccionar información del sistema una y otra vez. Esto constituye un problema dado que pueden existir artículos para los cuales el número de características que los definen sea muy elevado y en consecuencia, incomode la correcta interacción del usuario con el sistema, poniendo de manifiesto de nuevo cómo la alta dimensionalidad constituye un problema para los SR.

En definitiva, \begin{acrerror*}
{Algún signo de puntuación, please!!}{nuestro objetivo ha sido abordar el problema de la alta dimensionalidad en los SR haciendo uso de los conjuntos de implicaciones a través de un proceso de selección de atributos por parte del usuario mediante un SR conversacional que utilice características de los SR basados en contenido y en conocimiento.}
\end{acrerror*}
\begin{acrerror*}
{Creo que habría que comentar algo más de nuestra aproximación al problema, al menos decir que buscamos reducir el número de pasos (para que no quede ``descolgado'' lo  que comentas de esto en el siguiente párrafo)}{}
\end{acrerror*}

Un trabajo interesante en esta área es \cite{Jannach2009}, que establece la idoneidad de los enfoques basados en el conocimiento para los procesos conversacionales.
En particular, estos autores utilizan el razonamiento basado en restricciones, en lugar de nuestro enfoque basado en la lógica. Además, este trabajo trata sobre
concepto de optimización de consultas, análogo al aplicado en nuestra propuesta. Otro trabajo notable es \cite{TrabelsiWBR11}, que comparte nuestro objetivo de disminuir el número de pasos de la conversación. Los autores proponen métricas acerca del número de pasos de la conversación y tasas de poda, ambos muy similares a los utilizados en nuestro trabajo. Por otro lado, en \cite{Chen2007}, los autores demuestran cómo la posibilidad de que sea el usuario el encargado de la selección de atributos \begin{acrerror*}
{¿qué quiere decir ``supera'' aquí? Hay que ser más riguroso}{supera}
\end{acrerror*} al hecho de que sea el sistema mismo el encargado de dicha selección. Este hecho respalda nuestro enfoque en el cual el humano experto guía la conversación y el proceso de selección de características.


\section*{Evaluación de los sistemas de recomendación}
\label{seccion:evaluacionSistemasRecomendacion}
\begin{acrerror*}
{No sé si esto tiene entidad como para dedicarle una sección separada, quizás se pueda incluir en otra}{}
\end{acrerror*}

La evaluación de las predicciones y recomendaciones se ha convertido en un aspecto muy importante \cite{Herlocker2004,Burke2010}. Los SR requieren medidas de calidad y métricas de evaluación \cite{Gunawardana2009} para conocer la calidad de las técnicas, métodos y algoritmos para las predicciones y recomendaciones. Las métricas de evaluación \cite{HernandezdelOlmo2008} y los \textit{frameworks} de evaluación \cite{Bobadilla2011} facilitan la comparación de varias soluciones para el mismo problema.

No obstante, hay que tener en cuenta que \crerror*[inline]{}{\textst{dependiendo del SR con el que estemos trabajando,}} la evaluación habrá que llevarla a cabo utilizando aquellas métricas, que por su naturaleza y significado, \crerror*[inline]{sean coherentes con el}{\textst{tengan cabida en relación al}} SR que se desea evaluar. En nuestro caso, dado que hemos desarrollado un SR conversacional, \crerror*[inline]{son medidas adecuadas de rendimiento aquellas basadas en}{\textst{puede ser evidente que la primera medida que podemos aplicar es calcular}} el número de pasos \crerror*[inline]{de}{\textst{que se producen en}} la conversación \cite{McSherry01}. Por contra, otras métricas tan populares como son \textit{Precision} y \textit{Recall} \cite{Gunawardana2015} \begin{acrerror*}
{Esto hay que justificarlo, ¿por qué no son adecuadas? esto me lelva a que deberíamos haber dejado claro desde el principio que lo que hacemos nosotros no es exactamente recomendación sino filtrado. Para no tener que rehacer mucho de lo escrito, al menos debe dejarse claro desde el primer momento en que se presenta nuestra aportación que ésta se centra en la primera fase de la recomendación (el filtrado), y que no trabajamos en la aprte de ordenación de los resultados. }{no son adecuadas de aplicar.}
\end{acrerror*}


\section*{Aplicación desarrollada}
\label{seccion:aplicacionDesarrollada}
\begin{acrerror*}
{Insisto, no debemos presentar la aportación como un desarrollo de una aplicación software...}{}
\end{acrerror*}

La solución que hemos realizado consiste \crerror*[inline]{reducir los efectos del}{\textst{en gestionar el}} problema de la alta dimensionalidad mediante un proceso de selección de características guiado por el usuario (experto humano) dentro de un sistema conversacional \begin{acrerror*}
{Esta referencia aprece incompleta}{\cite{Yu2016}}
\end{acrerror*}. Para ello, una vez más haremos uso de una gestión inteligente de las implicaciones y de los conjuntos cerrados que nos va a permitir reducir sustancialmente el número de pasos necesarios en el diálogo entre el usuario y la aplicación para conseguir una recomendación adecuada en tiempo y forma. Para ello contaremos con el apoyo de la \crerror*[inline]{lógica }{\textst{ }}\slfde introducida por\crerror*[inline]{ l}{\textst{ }}os autores en \cite{Enciso2002} y\crerror*[inline]{, }{\textst{ }}en particular, se utilizará el algoritmo del cierre de atributos \slfde \cite{Mora2012} como núcleo de un marco de selección de atributos que será el que nos permita reducir el número de etapas del diálogo.

Además, se han realizado numerosas pruebas \crerror*[inline]{para contrastar la validez de la propuesta}{\textst{de aplicación sobre la propuesta que se ha desarrollado}}. En concreto, se han realizado pruebas utilizando información real sobre enfermedades y fenotipos como podemos apreciar en una de las contribuciones que avalan este trabajo de investigación \cite{Benito-Picazo2017}. Además, nuestra propuesta, al igual que la gran mayoría de los SR, permite actuar sobre diferentes \textit{datasets} de forma que podemos utilizar el mismo procedimiento para mejorar las recomendaciones conversacionales en diversos entornos. \begin{acrerror*}
{Esta frase es un poco redundante cono lo que se dice antes y después}{Esta versatilidad es una característica notoria, ya que permite libertad de maniobra en el caso de que se quieran introducir ciertos cambios, o simplemente que los datos sean diferentes.}
\end{acrerror*} En ese sentido, la propuesta de este trabajo casa con los conceptos de adaptabilidad y longevidad de los SR ya que el funcionamiento es independiente de la información de base con la que trabaje, sólo necesitamos conocer el conjunto de atributos e implicaciones subyacente a los datos.

Finalmente, \begin{acrerror*}
{Esto quizás quedaría mejor al principio y/o en las conclusiones}{antes de llegar a las últimas líneas del capítulo que dedicaremos a establecer el contenido del documento y las contribuciones producidas, cabe resaltar ya en este punto la naturaleza dual de la tesis, en el sentido de que mantendremos una línea de investigación en fundamentos teóricos complementada con la aplicación de dichos resultados en los dos campos de conocimiento mencionados anteriormente, bases de datos y sistemas de recomendación. Haremos especial hincapié en la parte aplicada del estudio con la intención de facilitar la transferencia de conocimiento a entornos diferentes del ámbito académico como el mercado empresarial}
\end{acrerror*}. Pasamos entonces ahora a \crerror*[inline]{describir}{\textst{desglosar}} la estructura del documento \crerror*[inline]{incluyendo las publicaciones que avalan la tesis}{\textst{y la producción conseguida}}.

%\vspace{2cm}
\newpage{}
\subsection*{Estructura de la Tesis}
\noindent
En este primer capítulo de introducción, hemos fijado los puntos fundamentales de la tesis, como son: el marco de trabajo sobre el que vamos a actuar, las técnicas que utilizaremos y los principales objetivos que se pretenden alcanzar. Concretamente, hemos estipulado la utilización de FCA y los conjuntos de implicaciones como base del estudio sobre la que aplicar técnicas basadas en la lógica para mejorar el tratamiento de la información.

A continuación, entraremos en el Capítulo \ref{cap:preliminares}, en el que hemos recopilado un conjunto de conceptos previos necesarios relacionados con: FCA, la lógica de simplificación, los sistemas de implicaciones y los operadores de cierre. Tras estos dos primeros capítulos, ya contaremos con el conocimiento previo necesario para abordar las dos partes principales de la tesis. 

Tras la introducción y los preliminares, pasamos a un tercer capítulo \ref{cap:clavesMinimales} en el que se presenta la primera contribución que avala este trabajo de investigación y que corresponde con el trabajo realizado en el campo de la búsqueda de las claves minimales. De forma general, este artículo presenta nuevos métodos para resolver el problema de la inferencia de claves minimales en esquema de datos basándose en la \crerror*[inline]{lógica}{\textst{}} \slfde y el uso de implicaciones. Además, \crerror*[inline]{se muestran}{\textst{refleja}} las implementaciones y las ventajas obtenidas al aplicar técnicas de computación paralela para poder aplicar los métodos sobre conjuntos de datos de \crerror*[inline]{un tamaño}{\textst{una entidad}} tal que  \crerror*[inline]{las}{\textst{}}técnicas secuenciales no son capaces de gestionar en cuanto a tiempo y recursos necesarios. Para ello se presentan los resultados obtenidos para los experimentos en entornos de supercomputación.

A continuación, \crerror*[inline]{el}{\textst{llegamos a un}} capítulo \crerror*[inline]{\ref{cap:clavesMinimales} se dedica}{\textst{análogo al anterior pero ahora para el tema de estudio referente}} a los generadores minimales. Este capítulo presenta un \crerror*[inline]{segundo}{\textst{nuevo}} artículo en el cual se lleva a cabo un estudio de los métodos de producción de generadores minimales basados en la lógica y el tratamiento de implicaciones. Se comprueba las mejoras de rendimiento de los métodos al aplicar reducciones en el espacio de búsqueda basadas en estrategias de poda. Al igual que en el caso de las claves minimales, se presentan las implementaciones paralelas de los métodos para poder tratar con conjuntos de datos de tamaño considerable y se incluyen las pruebas realizadas en entornos de supercomputación.

Como último capítulo dedicado a las aplicaciones desarrolladas mediante la gestión de implicaciones se presenta el capítulo \ref{cap:sistemasRecomendacion}. En este capítulo se incluye un novedoso \crerror*[inline]{trabajo}{\textst{artículo}} en el que se desarrolla una aproximación al tratamiento del problema de la dimensionalidad \crerror*[inline]{en}{\textst{que aparece en el campo de}} los sistemas de recomendación. En él, mediante el uso eficiente de la \crerror*[inline]{lógica}{\textst{ }} \slfd, las implicaciones y los operadores de cierre, \begin{acrerror*}
{Mejorar redacción}{se consigue un modelo de sistema de recomendación conversacional que es capaz de gestionar el problema de la dimensionalidad aliviando la sobrecarga de información con la que el usuario debe lidiar a la hora de obtener una recomendación por medio de un sistema conversacional. }
\end{acrerror*}Asimismo, se demuestra su \crerror*[inline]{su buen comportamiento mediante su evaluación con conjuntos de datos con}{\textst{utilidad en entornos reales mediante la realización de un experimento sobre}} información real.

Finalmente, cerraremos la tesis con \crerror*[inline]{el}{\textst{un último}} capítulo \ref{cap:conclusiones}\crerror*[inline]{, }{\textst{ }}dedicado a recopilar las principales conclusiones obtenidas y a proponer caminos por los que seguir ahondando en la investigación en esta materia. Además, se incluye una relación de las referencias consultadas y los respectivos índices de figuras, términos y tablas.

En aras de la completitud, se incluyen como anexos finales aquellos artículos que han sido publicados a lo largo de este periodo de investigación, que si bien no se utilizan como respaldo para esta tesis doctoral \crerror*[inline]{}{\textst{por no corresponder con los criterios de calidad exigidos a tal efecto}}, han sido la semilla y experiencia inicial \crerror*[inline]{a partir de la que se han desarrollado los}{\textst{para conseguir el resto de}} trabajos que \crerror*[inline]{}{\textst{sí}} actúan como aval.

\begin{acrerror*}
{La figura está muy chula, pero quizás quede mejor al principio de la sección ¿?}{En la Figura \ref{figura:esquemaTesis} se muestra de forma gráfica el contenido de la tesis y las contribuciones.}
\end{acrerror*}

\begin{figure}[htbp]
	\begin{center}
		\includegraphics*[width=1\textwidth,height=.8\textheight]{esquemaTesis.png}
	\end{center}
	\caption{Esquema de la estructura de la tesis y las publicaciones.}
	\label{figura:esquemaTesis}
\end{figure}
