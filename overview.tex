\pagestyle{empty}
\chapter*{Overview}
%\addcontentsline{toc}{chapter}{\protect{Overview}}

\markboth{Overview}{Overview}

\pagestyle{headings}

Information systems are experiencing a great expansion due, to a large extent, to the growth of the amount of information generated in today's society. It is therefore a challenge to investigate techniques that allow us to take advantage of and manage such amounts of information. In this framework, there exist numerous fields of knowledge such as: intelligent systems, big data \cite{Miao2014}, cloud computing, etc. Specifically, this doctoral thesis concerns to the field of information processing from the point of view of the analysis of formal concepts (FCA), with the intention of applying it in different areas of computer engineering such as databases and recommender systems.


FCA is a mathematical theory and a method for data analysis in terms of its relationships and structure. The main task of FCA is to define a method based on mathematics and logic that corresponds to the conceptual thinking of the human being. It is a technique with a solid formal basis \cite{Ganter1997} that can be applicable to a varied set of disciplines such as: data mining \cite{Stumme2002}, intelligent systems, machine learning \cite{Mitchell1997}, semantic web \cite{Wang2006}, software development, etc. FCA has established itself as a solid alternative to the process of acquiring and processing information for later use in applications using automatic reasoning techniques. More specifically, it has been recognized through a very significant increase of applications in very diverse areas, among which we can highlight biomedicine, tourism, education, semantic web, software development, biology, social networks, etc. Much of this growing interest lies in having a unique framework in which to develop, from the beginning to the end, the cycle that takes us from information to knowledge and, on it, to be able to perform tasks of automatic reasoning.

There are several techniques used with great success in tasks of data mining, classification, etc. But once these tasks are carried out, we must make use of other areas to work with the acquired knowledge. However, a big difference between a large part of such approaches and FCA is that the latter seeks to collect all the existing knowledge in the system. Thus, while in other areas the approximate term is habitual, here we find an important requirement in terms of semantic completeness. Obviously, this objective entails a great complexity and it is natural that the system is represented by reticles of large concepts or by systems of equally large implications. However, a great effort has been made lately in the development of efficient techniques and methods that have drawn the way in the framework of the classic FCA.

From the information available, the logical relationships obtained are those that are confirmed by our formal contexts, which can be considered as data tables that relate the existing information between a set of objects and a set of attributes. Mainly, there are two basic ways to represent the knowledge drawn from these tables: the concept grids and the implication sets.

For years, there have been studies in the literature \cite{Kuznetsov2002} where different algorithms have been researched and compared to obtain the concept grid from the data set. The difficulty of using concept grids is due to the fact that their size can reach levels that, in the worst case, suppose a very high computational cost for the methods in charge of their construction, which constitutes a limitation in the application of FCA.

Furthermore, the implications may be considered as rules of type \textit{if-then}, reflecting a very intuitive concept: when a premise is proved, a conclusion is met. Consequently, in FCA can be formalized as follows: Given $A$ and $B$ subsets of attributes, we would have the implication $A \rightarrow B$ if it is verified that $A' \in B'$, i.e. all objects that have each attribute of $A$ also have each attribute of $B$. With this definition, the implications obey the Armstrong's axioms \cite{Armstrong74} (reflective augmentative and transitive) common in functional dependencies that exist between the attributes of a database and considered as the first complete inference system for functional dependencies. Working with sets of implications allows us to apply automatic techniques based on logic. This stands as the main goal of this thesis, which mainly consists of applying logical mechanisms on sets of implications which allow us to develop methods for efficient data processing.

The approximation through logic is possible, in our case, thanks to valid and complete axiomatic systems such as the Armstrong's axioms \cite{Armstrong74} and the Simplification Logic \cite{Mora2004}. It will be these methods that, applied on sets of implications, provide us with the basis to work on the following three fields of knowledge: minimal keys, minimal generators and conversational recommender systems.


\subsection*{Aims and goals}
It has been intended that the thesis involves contributions that reflect the importance of the information contained within the data, the need for efficient treatment and how the theoretical mechanisms of FCA and the Simplification Logic are ideal tools to address these problems. Specifically, we have focused on the study of systems of implications based on FCA that allow improving the treatment of the information contained in data repositories, databases, etc., and how the techniques developed in this way allow improving aspects related to the bases of data and recommender systems. In particular, with respect to databases, this set of techniques will facilitate the discovery of keys and minimum generators of a data set. On the other hand, regarding recommender systems, by using these techniques of intelligent treatment of the set of implications of a given block of information, we will be able to lighten the recommendation process and improve the interaction with the user, alleviating the information overload.


Therefore, throughout the thesis, the theoretical methods have been researched and implemented in order to verify their efficiency and their adaptability to real models in databases and recommender systems. Concretely, we can say that the fundamental objective of this thesis is to approach hot problems in the field of information manipulation through methods that take advantage of the concept of implications system. These systems have the following general characteristics:
\begin{itemize}
	\item They are an optimal tool in terms of the expressiveness/computational cost ratio.
	\item They allow to use logic mechanisms.
	\item They can be extended (fuzzy systems \cite{Belohlavek2016}, etc.).
	\item They allow to be faced by means of FCA.
\end{itemize}

In this sense, we have to indicate that the development of the thesis has led to the achievement of objectives in a theoretical and practical way simultaneously. We can list them down as follows:

\begin{itemize}
	\item Theoretical aims. 
	\begin{itemize}
		\item Obtaining new methods of information processing through implications and FCA, as well as making progress on existing ones, which allow a more intelligent and efficient management of information.
		\item Research and proposal of new strategies for the discovery of keys and minimum generators from data tables.
		\item Research and proposal of new recommendation strategies that improve the quality of the recommendations and the interaction with the user.
	\end{itemize}
	\item Practical aims.
	\begin{itemize}
		\item Develop the necessary implementations and tests with the theoretical methods of information processing based on implications that allow to verify its performance on large amounts of information.
		\item Develop SR that benefit from these methods and are applicable to the current technological environment.
	\end{itemize}
\end{itemize}

Given the nature of the methods developed, in order to carry out large load tests such as those necessary to test the developed implementations, it will be necessary to have unusually high resources in computational terms. For this reason, a deep interaction work has been carried out with the Supercomputing and Bioinnovation Center of the University of Malaga, so that the resources available have allowed us to carry out the necessary tests.

Before going on to describe in a general way the balance of contributions obtained, at this point, we believe it is appropriate to establish a brief compendium of the disciplines that have had to come together in order to produce this doctoral thesis. In the first place, Logic appears as the fundamental basis to sustain the study. In addition, there is a considerable intervention of Mathematics, not only as a pillar for the logical basis, but also as a mechanism to carry out a large number of calculations and approximations when preparing the experiments. To the two previous ones, and of evident form given the nature of the thesis, the Computer Engineering is added. This last one takes part in a global form in the diverse aspects that have led the investigation to the practice. On the one hand, there is an important work of software engineering that has allowed to develop the necessary codes to put into practice the theoretical methods and to achieve the results that have led to the contributions. In particular, we must mention the programming work developed in high performance computing environments using low level languages. Additionally, there has been a deep immersion in the field of Information Systems, specifically, those related to both databases and recommender systems. Therefore, the joint interaction of aspects from all these disciplines has been what has led to culminate this research with the results presented in this doctoral thesis reflected in the section of contributions \ref{cap:aportaciones}.


\subsection*{Contributions to the field of keys and minimal generators}
In the first part of the thesis, we will study two fields of knowledge that, although different, are strongly related: minimal keys and minimal generators. We begin with the topic of minimal keys.

\vspace{0.3cm}

Finding all the minimal keys in a table is a difficult problem that provides many benefits in the design and optimization of the databases. Identifying properly the keys of a relational scheme is a crucial task for many current areas of information management, such as data modeling, query optimization, indexing, etc. The key concept is fundamental in any data model, including the relational data model of Codd \cite{Codd1970}.

Keys specify sets of attributes of a relation such that its projection uniquely identifies each tuple of the relation. A very important characteristic of the keys is their minimality. We denote a key as minimal when each attribute contained in its set of attributes is necessary to maintain the key property, that is, they are keys without superfluous attributes. Therefore, in the literature, the problem of minimal keys is to find all subsets of attributes that make up a minimal key given a set of functional dependencies that occur within a relational table.

% Therefore, each key is composed of a subset of attributes that plays the role of a {\ em domain} of a given function whose {\ em image} is the complete set of attributes. In this field, these functions, which in FCA appear as implications, are described by means of functional dependencies that specify a restriction between two subsets of attributes $ A $ and $ B $, it is denoted as $ A \ to B $, and they reflect a very intuitive idea, for any two tuples in a table, if they verify $ A $, they also check $ B $.

For years, there are several algorithms that use different classical techniques to address this problem \cite{Lucchesi78, Yu76, Zhang09, Armstrong74}. More recently, alternative methods have appeared using logic. In this thesis we will concentrate on those algorithms guided by logic, especially by the logic \slfde \cite{Enciso2002}, and more specifically, those that use the paradigm of Tableaux \cite{Morgan1992, Risch1992} to find the keys of a relational scheme using inference systems. The Tableaux paradigm can be considered a flexible and powerful framework for designing automated deduction methods to solve complex problems in an efficient way.

In previous works, several methods based on Tableaux \cite{Wastl98a, Wastl98} have been introduced. However, these methods generate extensive search spaces and, in many cases, the same solution (the same minimal key) appears at the end of several branches of the tree that represents the search space. These intrinsic characteristics of the Tableaux-based methods generate a strong limitation regarding the size of the problems that can be addressed, since for large problems, the search spaces can be unmanageable as has been demonstrated in previous works \cite{Cordero2013}.

However, a very interesting property within the search spaces induced by the Tableaux methods, is the total independence of its branches, so that each node of the search tree can be considered as a subproblem independent of the original problem. Thanks to this, working with these search trees provides us with an optimal way to build parallel algorithms capable of finding all the minimum keys of a table through parallel and independent processing.


In this way, our goal has been to design and implement search algorithms for minimal keys based on Tableaux that allow considerable amounts of information at the input. For this, codes have been implemented capable of operating on large quantities of hardware resources in parallel. To perform this task of parallel computing we have needed to use special computational resources. Specifically, we have made use of the resources available at the Supercomputing and Bioinnovation Center of the University of Malaga\footnote{https://www.scbi.uma.es/} without which it would have been impossible to carry out the vast majority of experiments since, in many cases, the solutions have been obtained after large periods of time of massive computing as we can see in \cite{Benito-Picazo2014, Benito-Picazo2016}.

In this sense, our main contribution is produced in the following way. Based on the axiomatic system of logic \slfde \ref{sec:logicaSimplificacion}, we propose a new method called \textit{Closure Keys} \ref{subsec:metodoCK} that incorporates an efficient pruning mechanism that uses the method of closure based in \slfde to improve the SST method \cite{CorderoEMG14}. The new method is based on the strong relationship between the notion of key and the closing operator defined in \cite{Mora2012}, which allows us to reduce the search space by making reductions on the way to the leaves, where we finally obtain the keys. In addition, several experiments are carried out between the parallel implementations of the SST method and the CK method (in supercomputing environments), to confirm the improvements that have been achieved, which are mainly a significant reduction in the number of search tree nodes and of the execution times as we can see in the contributions made in\cite{Benito-PicazoCMMSE2015, Benito-Picazo2016}.

\vspace{0.3cm}

We now go on to gather the contributions made in reference to the field of minimal generators.

The closed sets and the minimal generators are fundamental elements to build a complete representation of knowledge in FCA. Our intention has been to show how to use the inference system of \slfde \cite{Enciso2002, Cordero2012} and the output of the algorithm \cierree as an alternative to list all the closed sets and all the minimal generators of a set of attributes and implications at the input.

In this way, the implementations of the method to calculate the minimal generators and two evolutions of this one have been made, in which a new pruning strategy is going to propitiate that the results obtained in relation to the execution times and the magnitude of the search tree improve greatly. In addition, analogous to the case of minimal keys in dealing with substantial amounts of information, versions of the methods have been made so that they can be solved by masive computing techniques based on parallel strategies in the high computing environment offered by the Supercomputing and Bioinnovation Center of the University of Malaga\footnote{https://www.scbi.uma.es/}.

The calculation of the minimal generators is a problem that is not reduced to the theoretical field but is used to solve a multitude of problems. As a result, there are extensive studies in the literature on minimal generators, of which we can highlight \cite{Poelmans2013, Qu2007, Missaoui2012}.

For $ X,Y \subseteq M$ satisfying $X = Y^+_\Sigma$, it is said that $Y$ is a generator of the closed set $X$. As a consequence, any subset of $X$ containing $Y$ is also a generator of $X$. Since we are going to work with finite sets of attributes, the set of generators of a closed set can be characterized by those that are minimal. However, the enumeration of all closed sets and their minimal generators from a set of implications constitutes a complex problem with an exponential cost.

On our part, we are going to make use of the sets of implications as the basis of a method to list all the closed sets and their minimum generators. This method uses the logic \slfde and constitutes a considerable improvement of the one shown in \cite{Cordero2012}. The method generates a search tree, using the same Tableaux philosophy of the minimal keys, where the closed sets and their minimal generators are reflected. This method is called \ref{algoritmo:minGen} and is the seed for two new improved versions. First, the method \ref{algoritmo:minGenPr} incorporates a pruning mechanism to avoid the generation of minimal generators and redundant closures, and secondly, \ref{algoritmo:GenMinGen} proposes a generalization of the previous pruning strategy when considering the inclusion test of the subset not only with the information of the nodes of the same level, but also with all the minimal generators calculated up to that moment.

To demonstrate the performance of the methods, the necessary codes have been implemented for each one of them and a battery of input problems has been solved in order to show the performance achieved by each one of them. To evaluate this comparison, we will use the same two metrics as for the case of the minimal keys, that is, the execution time of the algorithm and the number of nodes in the generated search tree; and the justification is certainly the same.

After performing these experiments that can be solved locally in a conventional machine due to the low amount of information that contains the original files, the results tell us that \ref{algoritmo:minGen} is surpassed by \ref{algoritmo:minGenPr}, and in turn \ref{algoritmo:GenMinGen} improves the latter.

The numbers obtained for the sequential resolutions of the algorithms are very promising. However, as in the case of minimal keys, we want to be able to increase the size of the input problems and to do this, we return to parallel computing tasks. In this way, parallel versions of the codes of the methods are made and tested in supercomputing environments, demonstrating how parallelism is clearly an appropriate strategy to solve this type of problems and how it allows us to work with files with a greater amount of information. To support these statements about the results obtained, a series of tables and figures are included to easily appreciate the improvements achieved. The contributions made for this part can be found in \cite{Benito-PicazoCMMSE2017}.



\subsection*{Contributions to the field of conversational recommender systems}
In the second part of the thesis, we are going to introduce ourselves into the field of SR, and especially in the SR type called conversational \cite{Griol2018,Lee2017}. We will make use of the concepts of FCA and especially of the sets of implications and closing operators to achieve the development of a SR and address one of the most common problems that appear in these systems.

In a very basic way, we could consider SR as tools that group a wide range of information system techniques and applications with the intention of enhancing and favoring the user experience through recommendations. It is a field of knowledge with a broad trajectory since decades \cite{Hill1995,Adomavicius2005} but which is currently achieving a very considerable relevance due mainly to the expansion of new technologies that is favoring a greater approach to the majority of society due to its ability to make all kinds of recommendations on various elements available to everyone (books, music, jobs, etc.).

There are a large number of SR types, usually grouped according to the recommendation strategy they use. Among these types we can name the SR: content-based, knowledge-based, contextual, group, conversational, collaborative filtering, among several others. We can appreciate a more detailed classification in the book of Adomavicius and Tuzhilin \cite{AdomaviciusBook11}, which we can consider as one of the most important references in the field.

In the same way that there is such a wide range of possibilities as regards the types of SR that exist, it is natural that the number of problems that these systems have to face, both in their development and application phases, are numerous. We can compile a list with the most common such as: cold start, privacy, black sheep, scarcity, diversity, etc. In this list we can include a very common problem that a lot of SRs suffer from, it is the so-called problem of high dimensionality \cite{Salimi2017, Nagler2016}. This problem appears in the SR when it is necessary to work on \textit{datasets} with a high number of elements. Basically, we can say that this problem reflects the difficulty existing to elaborate a recommendation when there are many elements to decide between, but also when there are many characteristics that each element can fulfill.

As an intuitive example, suppose we want to choose a restaurant for dinner but we have a list of 100 places available, of which each of them can also offer a huge list of different services (type of kitchen, outdoor terrace, dining room, music live, handicapped access, price, etc.).

To address this problem, we can find many works in the literature on the reduction of the information dimension that can help us discard those characteristics that are not worthy of being considered according to different criteria. In \cite{Chen2007,Jannach2009,TrabelsiWBR11}, knowledge-based approaches are established for conversational processes, the number of steps in the conversation is taken into account, it is shown that if the user is in charge of the selection of attributes, the results are better than if the system itself rules such selection. This fact supports our approach in which the human expert guides the conversation and the process of feature selection.

For our part, the objective has been to address the problem of high dimensionality in SR through a process of selection of attributes by the user through a conversational SR that uses SR characteristics based on content and knowledge. For this, the developed SR (named \rs) will make an efficient treatment of the information making use of FCA, the implications and closing operators. Specifically, we will use the \slfde \cite{Enciso2002} and the closure algorithm \ref{algoritmo:Cls}.

\vspace{0.3cm}


In order to use \rs, we must keep in mind that the first thing we will need are \textit{datasets} with elements on which we can make recommendations. But in addition, these \textit{datasets} will have to contain the information according to a binary representation, that is, the value of each of the attributes associated with each element of the system will be a binary value that will represent if that element verifies that attribute or not. Once we have a \textit{dataset} in that format, we need the set of implications that are verified in the data and for this we will use already existing tools such as \cite{HuhtalaKPT99, YaoHB2002, Yevtushenko2006}. Once we have both \textit{dataset} as a set of implications, we are ready to proceed with \rs.

Given the conversational nature of \rs, the recommendation process for the user is developed through a dialogue. In this, the user will request features that he wants the item he is looking for (e.g. moderate price, if he is looking for a bicycle, Swedish cuisine, for a restaurant, resolution in megapixels, if he looks for a camera, etc.). After the feature selection by the user, the system executes its algorithm that performs two fundamental tasks, calculate the closure set corresponding to the user's selection and calculate the remaining set of implications. Upon completion and after launching a database query, the SR proposes a recommendation. The user can accept the recommendation or go ahead with the dialogue by adding new features that refine the result. Fortunately, at each step, you will not have to decide on the original set of possibilities, but now you only have to navigate between those that remain after the application of the algorithm, that is, those that are not logically implied by the current selection; relieving the information overload to the user.


The great achievement of using \rse with the algorithm \cierree is that we are reducing the number of characteristics or attributes and the number of implications \textit{simultaneously}, and therefore, in each new step of the dialog we do not need to extract the new set of implications, which is a task of \textit{data-mining} with exponential cost, but we can continue the interaction from here, where both attributes and implications have been reduced, and therefore with cost \textit{linear}.

Since SR research began, the evaluation of predictions and recommendations has become a very important aspect \cite{Herlocker2004, Burke2010}. Research in the field of SR requires quality measures and evaluation metrics \cite{Gunawardana2009} to know the quality of techniques, methods and algorithms for predictions and recommendations. The evaluation metrics \cite{HernandezdelOlmo2008} and the evaluation frameworks \cite{Bobadilla2011} facilitate the comparison of several solutions for the same problem. Taking into account the wide variety of factors that can intervene in the functioning of SR, the first task to be carried out is to evaluate the options to find out which metrics are the ones that best adapt to the SR that we want to evaluate.

In our case, since we are dealing with a conversational SR, it may be evident that the first measure we can apply is to calculate the number of steps that occur in the conversation \cite{McSherry01}. On the other hand, other metrics so popular in SR as \textit{Precision}, \textit{Recall} \cite{Gunawardana2015}, MAE, RMSE, etc., are not suitable to apply in our case because we would always obtain maximum values in both metrics. This is because we are dealing with implications, and therefore, we are not dealing with an approximate but ensuring that the reliability is 100\%.

Therefore, in addition to calculate the number of steps of the conversation, two additional evaluation measures have been defined to measure the performance of the system, so that the list of evaluation measures used on \rse goes as follows:
\begin{enumerate}
	\item \textbf{Number of steps.} Shows the number of steps that are given in the dialog and consequently, the number of attributes selected by the user. It provides us with a clear vision of whether the system has required much or little interaction to satisfy the user's demand.
	\item \textbf{Pruning velocity.} Evaluates the percentage of attributes that the system frees the user to take into account throughout the conversation and cumulatively from one step to the next. Reflects if pruning ratios of the algorithm are better at the beginning of the conversation or in later steps.
	\item \textbf{Attributes reduction.} Reports the overall reduction of attributes that the system has made at the end of the dialogue.
\end{enumerate}

Once we had the defined system together with its operation and the evaluation metrics that are going to be used, we went on to elaborate a wide series of experiments to verify the viability, the utility and the benefit that we obtain with the work developed \ref{cap:experimentosSR}.

The experiments chapter is really important for several reasons. The first is obvious, to validate the study and the development carried out; but the experiments carried out also serve to show us the effective journey from the theoretical part to the practice that has ended with a real application of engineering.

There have been experiments developed over information, we could call test, and others over real information. The results show how the developed system fulfills its mission and also surpassess other existing proposals in the literature \cite{TrabelsiWBR11}. In this sense, we have worked and evaluated \rse on \textit{datasets} that have been a reference when testing the operation of these systems such as MovieLens' \textit{datasets}.


Additionally, since many of the \textit{datasets} available on the web needed some pre-treatment to adapt to the accepted \rs format, we chose to build our own \textit{datasets} from information available on the web. For this, \textit{datasets} were created, such as Costa del Sol Hotels \ref{seccion:hotelesCostaDelSol}, World POIs \ref{seccion:POIsMundial}, or Diseases and Symptoms \ref{seccion:sintomasDataset}, the latter used in one of the contributions that support the thesis \cite{Benito-Picazo2017}.

A test consisting of simulating 100 dialogues between the user and the system was carried out and the aforementioned metrics were applied to each of them so the results obtained were collected. In each case, a brief compendium is made as a conclusion where the improvements are recorded and progress is justified over other alternatives. Mainly, we can say that we have managed to take a step forward to address the problem of the high dimensionality that appears in the SR thanks to the treatment we perform on the data through the use of the implication sets and the closing operators.
